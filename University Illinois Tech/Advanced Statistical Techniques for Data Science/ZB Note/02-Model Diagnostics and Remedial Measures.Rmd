---
title: "Model Diagnostics and Remedial Measures - Note"
author: Zehui Bai 
date: "2025-08-18"
output:
  html_document:
    df_print: paged
    number_sections: no
    toc: yes
    toc_float:
      collapsed: true
      smooth_scroll: true
    theme: journal # <!-- https://bootswatch.com/3/  -->
    highlight: tango
    code_folding: "hide" 
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
 

## 《Regression Diagnostics》

 
**回归诊断的目的**
回归模型在理论上依赖一系列假设，比如线性关系、同方差性、误差独立性和正态性。但在真实数据中，这些假设往往难以完全满足。回归诊断的目标就是检验这些假设，并帮助我们发现模型可能存在的问题。

---

线性回归模型的主要假设
在经典的线性回归模型

$$
y_i = \beta_0 + \beta_1 x_{i1} + \cdots + \beta_k x_{ik} + \epsilon_i
$$

中，通常有以下假设：

* 因变量与自变量之间存在线性关系。
* 误差的均值为零，即 $E(\epsilon_i)=0$。
* 误差具有同方差性，即 $\text{Var}(\epsilon_i)=\sigma^2$。
* 误差服从正态分布。
* 各误差之间相互独立。
* 自变量没有测量误差，并且相互线性独立。

这些假设决定了模型的统计推断是否有效。

---

残差与诊断图
在拟合回归模型后，得到预测值 $\hat{y}_i$，残差定义为 $e_i = y_i - \hat{y}_i$。
诊断回归模型时，常通过 R 语言中的 `plot(lm_model)` 绘制四类图形：

1. 残差对拟合值图（Residuals vs Fitted Values）

   * 用于检验线性关系和同方差性。
   * 理想情况是残差均匀散布在零水平线周围。
   * 如果出现漏斗状，说明存在异方差；若呈现弯曲趋势，说明模型可能遗漏了非线性关系。

2. 尺度-位置图（Scale-Location Plot）

   * 横轴为拟合值，纵轴为 $\sqrt{|e_i|}$。
   * 主要检查残差方差是否恒定。
   * 理想状态下，点应随机分布在水平直线附近。若出现趋势，则表明误差方差随拟合值变化。

3. Q-Q 图（Normal Q-Q Plot）

   * 检查残差是否近似服从正态分布。
   * 如果点基本落在 45 度参考线上，则近似正态。若呈现 S 形或明显弯曲，说明残差分布偏离正态性。

4. 残差对杠杆值图（Residuals vs Leverage, Cook’s Distance）

   * 用于检测异常点和高影响点。
   * 如果某个点位于右上角或右下角并超出 Cook’s distance 的虚线边界，说明它可能对回归系数产生过大影响，需要进一步关注。

---

总结
回归诊断的核心是用图形化方法检验假设是否成立。

* 残差图：检查线性关系和方差齐性。
* 尺度-位置图：检查误差方差是否恒定。
* Q-Q 图：检验误差正态性。
* 杠杆值图：识别离群点和有影响的观测值。

一旦发现问题，可以考虑对数据进行变换（如对数变换、Box-Cox 变换）、加入非线性项或交互项，或者采用稳健回归、广义线性模型等替代方法。
 


## 《Variance-Stabilizing Transformations（方差稳定化变换）》 

**1) 背景与目标：为什么需要方差稳定化变换**
在线性回归中，我们常假设误差项满足：均值为 0、方差为常数（同方差/齐性，homoscedasticity）、独立同分布且服从正态分布，解释变量线性独立并且无测量误差等。如果数据违反了**同方差**假设（例如残差随拟合值增大而增大），经典线性模型估计与推断的可靠性会下降。文档的核心就是：**在响应变量上做恰当变换，使其方差“尽量不随均值变化”，从而“稳定方差”**。

典型线性回归假设回顾（简化表述）：

* 关系是线性的；误差均值为 0；误差方差为常数；误差正态；误差独立；自变量非随机且无测量误差，彼此线性独立。

---

**2) 形式化推导：如何得到“合适的变换”**
设随机变量 $y$ 的均值与方差满足

$$
\mathbb E(y)=\mu,\quad \mathrm{Var}(y)=g(\mu),
$$

其中 $g(\cdot)$ 是某个（未知或可近似）的函数。写成 $y=\mu+e$（$\mathbb E(e)=0$，$\mathrm{Var}(e)=g(\mu)$）。我们希望找到一个变换 $h(\cdot)$，使得 $h(y)$ 的方差尽量为常数。对 $h(y)=h(\mu+e)$ 在 $\mu$ 处作一阶泰勒展开：

$$
h(\mu+e)\approx h(\mu)+e\,h'(\mu),
$$

于是

$$
\mathbb E[h(y)]\approx h(\mu),\quad \mathrm{Var}[h(y)]\approx [h'(\mu)]^2\,g(\mu).
$$

要让 $\mathrm{Var}[h(y)]$ 不依赖 $\mu$，最自然的选择是令

$$
[h'(\mu)]^2\,g(\mu)=1\quad\Rightarrow\quad h'(\mu)=\frac{1}{\sqrt{g(\mu)}}.
$$

积分可得

$$
h(\mu)=\int^\mu \frac{1}{\sqrt{g(s)}}\,\mathrm ds.
$$

这给出了**方差稳定化变换的一般构造**：只要能近似知道“方差如何随均值变化”的函数 $g$，就可以通过上式得到 $h(\cdot)$。

---

**3) 例子：计数型响应（泊松分布）与 $\sqrt{y}$ 变换**
文档举了一个示例：令 $y$ 为致死车祸次数，$x$ 为撞击速度。假设

$$
y=\beta_0+\beta_1 x+\varepsilon,\quad y\sim \text{Poisson}(\lambda).
$$

对泊松分布，$\mathrm{Var}(y)=\mathbb E(y)=\lambda$。同时 $\mathbb E(y)$ 又随 $x$ 增大而增大（在该示意模型中），因此**方差也随 $x$ 增大**，违背同方差。此时 $g(\mu)=\mu$ 是恒等函数，由上一节的公式，

$$
h(\mu)=\int^\mu \frac{1}{\sqrt{s}}\,\mathrm ds = 2\sqrt{\mu}\quad\Rightarrow\quad h(y)\propto \sqrt{y}.
$$

于是对响应做 **平方根变换**（常用形式是 $y'=\sqrt{y}$），再拟合 $\sqrt{y}=\beta'_0+\beta'_1 x$。文档配套的诊断图（残差 vs 拟合值、尺度-位置图）显示：变换前呈现明显异方差；变换后更接近同方差。**平方根变换是处理泊松型计数数据异方差的经典选择**。

---

**4) 常见均值–方差关系与对应的“经验型”变换**
文档列出了一个对照表，给出几类常见的“方差与均值的关系”和相应的变换（近似意义下）：

* 若 $\sigma^2 \propto \text{constant}$：不用变换，$y' = y$。
* 若 $\sigma^2 \propto \mathbb E(y)$：用平方根，$y'=\sqrt{y}$（泊松型计数常见）。
* 若 $\sigma^2 \propto \mathbb E(y)\{1-\mathbb E(y)\}$：用反正弦平方根变换，$y'=\sin^{-1}\!\big(\sqrt{y}\big)$（比例/概率型数据常见；更标准的场景是二项分布中的样本比例 $\hat p$）。
* 若 $\sigma^2 \propto [\mathbb E(y)]^2$：用对数变换，$y'=\ln(y)$（乘法性误差、相对误差更稳定时常见）。
* 若 $\sigma^2 \propto [\mathbb E(y)]^3$：用 $y^{-1/2}$。
* 若 $\sigma^2 \propto [\mathbb E(y)]^4$：用 $y^{-1}$。

上表可以看作**Box-Cox 家族思想**的一种具体化：当方差随均值呈幂律增长时，采用相应幂次的变换能在一阶近似下稳定方差。**需要强调**：这些是**经验/近似**变换，真实效果要靠诊断图验证。

---

**5) 实践流程：如何把 VST 用到你的回归里**
文档建议把 VST 作为一种**迭代的、基于诊断的试错过程**来用：

1. **初始建模**：先用原始 $y$ 建一个线性模型（或合适的基线模型）。
2. **检查诊断**：重点看“残差 vs 拟合值”和“尺度-位置（Scale-Location）”图。如果残差的散布随拟合值增大而“喇叭口”式扩张，多半是异方差。
3. **选择候选变换**：根据你的数据类型与对 $g(\mu)$ 的认识（或参考“常见关系–变换表”），挑一两种变换（如 $\sqrt{y}$、$\log y$ 等）对响应做变换。
4. **重拟合并复查诊断**：比较变换前后诊断图，选择能明显改善异方差且残差更接近“独立同方差正态”的方案。必要时再微调（例如从 $\sqrt{y}$ 到 $\log(y+c)$ 之类）。
5. **解释与回译（可选）**：若需要在原量纲解释，可将预测的 $h(y)$ 结果再“回变换”到 $y$ 的尺度上，同时注意变换带来的偏差修正问题（例如对数变换下的 **Duane/“smearing”** 校正思想，虽然文档没有展开）。

这种“**变换—诊断—再变换**”的循环，是传统线性模型里处理异方差与偏态的常规武器。

---

**6) 与广义线性模型（GLM）的关系（补充说明）**
虽然文档以线性回归 + 变换为主线，但你可能注意到：泊松计数型响应在 GLM 里常用**对数链接**并建模 $\log(\mu)=\beta_0+\beta_1 x$。**VST 与 GLM 是两条路径**：

* **VST 路径**：变换响应后，用常规最小二乘；优点是直观，缺点是近似性、解释回译麻烦。
* **GLM 路径**：显式指定均值–方差结构与链接函数，直接在均值层建模；优点是统计性质清晰，常更稳健。
  文档旨在教授“当你坚持线性回归框架时，如何用 VST 修正异方差”，这与 GLM 并不矛盾。

---

**7) 小结：你应当记住的要点**

* **核心思路**：当 $\mathrm{Var}(y)$ 随 $\mathbb E(y)$ 变化时，通过 $h'(\mu)=1/\sqrt{g(\mu)}$ 构造变换 $h$，近似让 $\mathrm{Var}[h(y)]$ 为常数。
* **常用配方**：$\sqrt{y}$（泊松/计数）、$\log y$（方差与均值平方成正比）、$\sin^{-1}\sqrt{y}$（比例/概率数据）等。
* **实践方法**：以诊断图为导向，试错选择变换，直到残差图显示更接近同方差。这是一种**经验性**但效果常很好的方法。

## 《Box-Cox Transformation》 
 
**1. 回归模型的基本假设**
在经典线性回归模型中，我们通常假设：

* 响应变量与自变量之间是 **线性关系**。
* 误差项 $\epsilon_i$ 的期望为 **零**。
* 误差项具有 **常数方差**（齐性，homoscedasticity）。
* 误差项服从 **正态分布**。
* 误差项之间 **互不相关**。
* 自变量是 **非随机的**，并且 **无测量误差**。
* 自变量之间 **线性无关**。

当这些假设不满足时（尤其是误差方差不齐或响应变量不服从正态），回归模型的推断会受到影响。

---

**2. Box-Cox 变换的提出**
Box 和 Cox（1964）提出了一个方法：通过对响应变量 $y$ 做 **幂变换（power transformation）** 来改善模型假设。

形式为：

$$
y_i^{(\lambda)} = 
\begin{cases} 
\dfrac{y_i^\lambda - 1}{\lambda}, & \lambda \neq 0 \\[6pt]
\ln y_i, & \lambda = 0 
\end{cases}
$$

变换后的模型：

$$
y_i^{(\lambda)} = \beta_0 + \beta_1 x_{i1} + \cdots + \beta_k x_{ik} + \epsilon_i
$$

其中，参数 $\lambda$ 和回归系数 $\beta$ 一起通过 **最大似然估计** 求得。

---

**3. Box-Cox 变换包含的常见形式**
不同的 $\lambda$ 对应常见的数据变换：

* $\lambda = 2$ → $y' = y^2$
* $\lambda = 1/2$ → $y' = \sqrt{y}$
* $\lambda = 0$ → $y' = \ln y$
* $\lambda = -1/2$ → $y' = 1/\sqrt{y}$
* $\lambda = -1$ → $y' = 1/y$

注意：Box-Cox 仅适用于 **正值的响应变量**。若数据中有负值或零，需要先做平移处理（例如整体加一个常数）。

---

**4. R 语言中的实现**
在 R 中可以通过 **MASS 包** 的 `boxcox()` 函数实现。

* `boxcox(model1)` 会绘制 λ 的对数似然函数图，并给出最佳 λ 的 95% 置信区间。

* 例如：

  ```R
  bc = boxcox(model1)
  best.lam = bc$x[which(bc$y == max(bc$y))]
  ```

  得到最优 λ。

* 如果最优 λ = 0.626…，而 0.5 落在置信区间内，可以选择简单的平方根变换 $y' = \sqrt{y}$，既合理又易解释。

---

**总结**：
Box-Cox 变换是用来处理回归模型中 **非正态误差分布** 和 **方差不齐性** 的一种方法。它通过幂变换使响应变量更符合模型假设，从而提升估计与推断的可靠性。


## 《Transformations to Linearize the Model》

 

**模型线性化的变换**
作者：Dr. Kiah Wah Ong

**回顾线性回归模型的主要假设**
在经典线性回归模型

$$
y_i = \beta_0 + \beta_1 x_{i1} + \cdots + \beta_k x_{ik} + \epsilon_i, \quad i=1,2,\dots,n
$$

中，假设包括：

* 响应变量与自变量的关系是**线性的**；
* 误差项 $\epsilon_i$ 的均值为 **0**；
* 误差项具有**常方差**（同方差性）；
* 误差项 $\epsilon_i$ 服从**正态分布**；
* $\epsilon_i$ 与 $\epsilon_j$ 相互独立（$i\neq j$）；
* 自变量 $x_1,\dots,x_k$ 是非随机且**无测量误差**的；
* 自变量之间是**线性独立**的。

---

**为什么需要变换？**
虽然回归分析的出发点是“假设 $y$ 和自变量线性相关”，但有时这个假设并不合适：

1. 在模型诊断中发现了**非线性**；
2. 根据**先验经验或理论**判断关系应为非线性。

这时，可以尝试**线性化变换**：通过对 $y$ 或 $x$ 做适当的函数变换，把一个非线性模型转化为线性形式。

---

**常见的可线性化模型**

1. **幂函数模型**

   $$
   y = \alpha x^{\beta}
   $$

   取对数：

   $$
   \log y = \log \alpha + \beta \log x \quad \Rightarrow \quad y' = \beta_0 + \beta_1 x'
   $$

   （这里 $y' = \log y, x' = \log x$）

2. **指数函数模型**

   $$
   y = \alpha \beta^x
   $$

   取对数：

   $$
   \log y = \log \alpha + (\log \beta) x
   $$

3. **对数线性模型**

   $$
   y = \beta_0 + \beta_1 \log x
   $$

   设 $x' = \log x$，得到：

   $$
   y = \beta_0 + \beta_1 x'
   $$

4. **倒数模型**

   $$
   y = \frac{x^{\beta_0}}{x^{\beta_1}} = x^{\beta_0} x^{-\beta_1}
   $$

   取倒数：

   $$
   y' = \frac{1}{y}, \quad x' = \frac{1}{x} \quad \Rightarrow \quad y' = \beta_0 - \beta_1 x'
   $$

---

**总结表格（常见变换）**

| 原始关系                           | 变换方式                       | 线性化后的模型                          |
| ------------------------------ | -------------------------- | -------------------------------- |
| $y = \beta_0 x^{\beta_1}$      | $y' = \log y, x' = \log x$ | $y' = \log \beta_0 + \beta_1 x'$ |
| $y = \beta_0 e^{\beta_1 x}$    | $y' = \ln y$               | $y' = \ln \beta_0 + \beta_1 x$   |
| $y = \beta_0 + \beta_1 \log x$ | $x' = \log x$              | $y = \beta_0 + \beta_1 x'$       |
| $y = x^{\beta_0} x^{-\beta_1}$ | $y' = 1/y, x' = 1/x$       | $y' = \beta_0 - \beta_1 x'$      |

---

**示例**
假设我们有数据集 `VT1.CSV`，观测到 $x$ 与 $y$ 的关系是非线性的。
如果先验经验告诉我们关系应为：

$$
y = \alpha \beta^x
$$

则取自然对数：

$$
\ln y = \beta_0 + \beta_1 x + \epsilon
$$

用 R 建立模型后，诊断图显示拟合效果良好，说明通过变换成功把非线性关系转化为线性关系。
 
 
## 《Weighted Least Squares, WLS》
 

**主题：加权最小二乘法（Weighted Least Squares, WLS）**

文件由 Dr. Kiah Wah Ong 编写，主要介绍如何在回归模型中处理误差项方差不恒定（异方差性，heteroscedasticity）的情况。

---

**1. 回归模型的主要假设**
在线性回归模型中：

$$
y_i = β_0 + β_1x_{i1} + \cdots + β_kx_{ik} + \epsilon_i
$$

常见假设包括：

* 响应和自变量的关系是线性的；
* 误差项均值为零；
* 误差项方差恒定（同方差性）；
* 误差项独立且正态分布；
* 自变量是确定的并且无测量误差；
* 自变量线性无关。

---

**2. 问题：异方差性**
如果误差项 $\epsilon_i$ 独立但方差不同（即 $\text{Var}(\epsilon_i) \neq \sigma^2$），除了采用方差稳定化变换外，加权是另一种有效方法。
特别适合在我们 **已知或大致了解方差结构** 时使用。

---

**3. 示例模型**
考虑：

$$
y = β_0 + β_1x + \epsilon, \quad \epsilon \sim N(0, x\sigma^2)
$$

即 $\text{Var}(y_i) = x_i\sigma^2$，方差随着 $x$ 增大而增大。

---

**4. 转换与加权思想**
通过除以 $\sqrt{x_i}$，使误差方差稳定：

$$
\frac{y_i}{\sqrt{x_i}} = \frac{β_0}{\sqrt{x_i}} + β_1\sqrt{x_i} + \epsilon'_i
$$

其中 $\epsilon'_i \sim N(0,\sigma^2)$。

于是目标函数变为：

$$
S(β_0, β_1) = \sum_{i=1}^n \frac{1}{x_i}(y_i - β_0 - β_1x_i)^2
$$

这就是加权最小二乘法，其中权重 $w_i = \frac{1}{x_i}$。

---

**5. 参数估计公式**
引入加权均值：

$$
x^{(w)} = \frac{\sum w_i x_i}{\sum w_i}, \quad y^{(w)} = \frac{\sum w_i y_i}{\sum w_i}
$$

定义：

$$
S^{(w)}_{xx} = \sum w_i(x_i - x^{(w)})^2, \quad S^{(w)}_{xy} = \sum w_i(x_i - x^{(w)})(y_i - y^{(w)})
$$

则估计量为：

$$
\hat{β}_1 = \frac{S^{(w)}_{xy}}{S^{(w)}_{xx}}, \quad \hat{β}_0 = y^{(w)} - \hat{β}_1x^{(w)}
$$

---

**6. 可视化与结果**

* 残差 vs. 拟合值图中，普通最小二乘（OLS）会显示漏斗形模式（方差随 $x$ 增大）。
* 加权后，残差的方差被稳定下来。

标准化残差 = 残差 ÷ 其标准差。

---

**7. 统计推断与区间**
WLS 下，统计推断（如置信区间、预测区间）与 OLS 类似，但需要带上权重信息。

---

**8. 推广到多元回归**
对于多元回归：

$$
y_i = β_0 + β_1x_{i1} + \cdots + β_kx_{ik} + \epsilon_i, \quad \epsilon_i \sim N(0, \sigma^2_i)
$$

若 $\sigma_i^2 = \frac{\sigma^2}{w_i}$，则 WLS 目标函数为：

$$
S(β_0, \dots, β_k) = \sum_{i=1}^n w_i \left(y_i - β_0 - \sum_{j=1}^k β_j x_{ij}\right)^2
$$

其解为：

$$
\hat{β}_{WLS} = (X^T W X)^{-1} X^T W y
$$

其中 $W = \text{diag}(w_1, \dots, w_n)$。
 
 
## 《Autocorrelation》
 

**自相关 (Autocorrelation)**

**回顾回归模型的基本假设：**
在经典线性回归模型

$$
y_i = \beta_0 + \beta_1 x_{i1} + \cdots + \beta_k x_{ik} + \epsilon_i, \quad i = 1, 2, \dots, n
$$

中，我们通常假设：

* 响应变量和自变量之间是线性关系。
* 误差项 $\epsilon_i$ 的期望为 0。
* 误差项方差恒定（同方差性）。
* 误差项服从正态分布。
* 误差项之间互不相关。
* 自变量是非随机的，且测量无误差。
* 自变量之间线性独立。

**自相关的定义：**

* 要求误差项 $\epsilon_i$ 与 $\epsilon_j$ 在 $i \neq j$ 时不相关，即

  $$
  \text{cov}(\epsilon_i, \epsilon_j) = 0
  $$
* 如果误差项之间存在相关性，尤其是带有时间顺序的数据，就会出现“自相关”。

**自相关的常见场景：**

* 处理时间序列数据时很容易出现（例如金融市场、气象、销售量随时间变化）。

**自相关带来的问题：**

1. 最小二乘估计仍然无偏，但效率下降（不再是最小方差估计）。
2. 方差估计可能被低估 → 给出虚假的“模型拟合良好”印象。
3. 置信区间和显著性检验结果不再准确。

---

**Durbin-Watson 检验 (DW 检验)：**

* 假设误差项服从一阶自回归过程 (AR(1))：

  $$
  \epsilon_i = \phi \epsilon_{i-1} + a_i
  $$

  其中 $a_i \sim NID(0, \sigma^2)$，且 $|\phi| < 1$。
* 对应的回归模型：

  $$
  y_i = \beta_0 + \beta_1 x_i + \epsilon_i
  $$

**误差项的性质：**

* $\text{E}(\epsilon_i) = 0$
* $\text{Var}(\epsilon_i) = \frac{\sigma^2}{1-\phi^2}$
* $\text{Cov}(\epsilon_i, \epsilon_{i-s}) = \phi^s \cdot \text{Var}(\epsilon_i)$
* 一阶自相关系数 $\rho_1 = \phi$，更高阶 $\rho_k = \phi^k$。

**DW 检验的假设：**

* $H_0: \phi = 0$ （无自相关）
* $H_1: \phi > 0$ （存在正自相关）
* 检验统计量：

  $$
  d = \frac{\sum_{i=2}^n (\epsilon_i - \epsilon_{i-1})^2}{\sum_{i=1}^n \epsilon_i^2}
  $$
* 直观解释：如果相邻残差很相似（高度相关），则 $(\epsilon_i - \epsilon_{i-1})^2$ 很小，导致 $d$ 取值小。

  * $d \approx 2(1-\hat{\phi})$
  * 当 $d \approx 2$ → 无自相关
  * 当 $d \to 0$ → 强正相关
  * 当 $d \to 4$ → 强负相关

**决策规则：**

* 若 $d < d_L$，拒绝 $H_0$，存在自相关。
* 若 $d > d_U$，不拒绝 $H_0$。
* 若 $d_L \leq d \leq d_U$，结论不确定。

---

**其他检测方法：**

* **ACF 图 (Autocorrelation Function plot)**：绘制残差在不同滞后下的相关性，若残差存在明显延迟相关，说明有自相关。

---

**自相关的解决方法：**

1. 改进模型 → 检查是否遗漏了重要自变量。
2. 改进实验/数据收集方式 → 消除时间、空间或顺序上的依赖。
3. 做适当变量变换。
4. **一阶自回归修正 (AR(1) Correction)**：

   * 转换：

     $$
     y'_i = y_i - r y_{i-1}, \quad x'_i = x_i - r x_{i-1}
     $$

     （其中 $r \approx \rho_1$）
   * 然后对 $(y'_i, x'_i)$ 做普通最小二乘回归。
   * 最终修正系数：

     $$
     \beta_0 = \frac{\beta'_0}{1-r}, \quad \beta_1 = \beta'_1
     $$

---

👉 总结：

* 自相关主要出现在时间序列数据中。
* 它不会影响回归系数的无偏性，但会严重影响标准误、显著性检验和模型的可靠性。
* Durbin-Watson 检验是常用方法，配合 ACF 图进行验证。
* 若存在自相关，可通过改进模型或 AR(1) 修正来解决。
 
 
## 《Multicollinearity》
 
 
**回归模型的基本假设**
在线性回归中，我们通常假设：

* 响应变量和自变量之间是线性关系。
* 误差项均值为零。
* 误差具有相同的方差（同方差性）。
* 误差服从正态分布。
* 不同观测的误差不相关。
* 自变量是非随机的，且无测量误差。
* 自变量线性独立。

**多重共线性（Multicollinearity）**
多重共线性是指自变量之间高度相关（线性相关）。在回归中，参数估计需要矩阵 $X^TX$ 可逆，如果自变量高度相关，这个矩阵可能接近不可逆，从而导致问题。

举例：
如果模型是
$\hat{y} = \hat{\beta}_0 + \hat{\beta}_1 x_1 + \hat{\beta}_2 x_2 + \hat{\beta}_3 x_3$，
但 $x_3 = 4x_1 + 3x_2$，那么 $x_3$ 没有提供新信息，反而导致参数估计不稳定。

**多重共线性的影响**

1. 参数估计的方差变大：
   当两个变量高度相关（相关系数接近 ±1），参数估计的方差会趋向无穷大。
   这意味着用不同样本训练模型，系数可能差异极大。

2. 系数被放大：
   在强多重共线性下，估计系数 $\hat{\beta}$ 的长度往往比真实值大得多，也就是说模型倾向于夸大回归系数。

**多重共线性的诊断方法**

* 通过散点图或相关矩阵，直观判断变量之间是否相关。
* 通过方差膨胀因子（Variance Inflation Factor, VIF）：
  对每个变量 $x_j$，用其他变量回归它，得到 $R^2_j$，然后计算

  $$
  VIF_j = \frac{1}{1 - R^2_j}
  $$

  * 如果 $VIF_j = 1$，说明与其他变量不相关。
  * 如果 $VIF_j > 4$，需要关注。
  * 如果 $VIF_j > 10$，说明共线性严重，应采取措施。

**解决多重共线性的方法**

* 收集更多数据，减少变量之间的高度相关性。
* 模型重新设定：

  * 通过构造新变量（组合、比率）减少依赖。
  * 删除冗余变量。

**岭回归（Ridge Regression）**

* 普通最小二乘在多重共线性下会导致系数不稳定。
* 岭回归通过在目标函数中加入惩罚项

  $$
  S = \sum_{i=1}^n (y_i - \beta_0 - \sum_{j=1}^k \beta_j x_{ji})^2 + \lambda \sum_{j=1}^k \beta_j^2
  $$

  来缩小系数，减少方差。
* 惩罚强度由 $\lambda$ 控制，通常通过交叉验证（GCV）选择。
* 在实施岭回归前，需要对自变量做标准化（否则不同量纲会导致不公平惩罚）。

**总结**

* 多重共线性会导致模型参数不稳定，方差大，系数估计值被夸大。
* 可以通过散点图、相关矩阵、VIF 来诊断。
* 解决方法包括收集更多数据、删减或变换变量，以及采用岭回归等正则化方法。
 
 
## 《Variable Selection and Model Validation》


**变量选择与模型验证**

**引言**
在前面的回归分析中，我们通常假设：

* 所有的预测变量（自变量）都已经事先确定；
* 我们对模型的基本形式有清晰认识。

但在实际情况中：

* 可能存在很多候选预测变量，需要决定哪些要包含在模型中；
* 可能会有多个候选模型都能通过诊断检验，需要选择最佳模型。

这就引出了 **变量选择 (Variable Selection)** 和 **模型验证 (Model Validation)** 的问题。

---

**变量选择 (Variable Selection)**

* **定义**：从一组候选预测变量中选出合适的子集。
* 在临床试验等严格设计的研究中，变量选择通常不是必要的；
* 在观察性研究中，由于收集了大量潜在变量，不清楚哪些是重要的，因此需要变量选择。

目标是确定“最佳”的预测变量。

**常见方法**

1. **全子集搜索 (Full Search)**

   * 如果有 $k$ 个预测变量，那么可能的回归模型数目是 $2^k$。
   * 例如 3 个变量（x1, x2, x3） → 共有 8 个模型可考虑。
   * 但随着变量数增加，计算量会迅速爆炸。

2. **逐步回归 (Stepwise Regression)**

   * **前向选择 (Forward Stepwise)**：从无变量开始，每次添加一个显著的变量。
   * **后向选择 (Backward Stepwise)**：从全模型开始，每次去掉一个不显著的变量。
   * 在选择过程中要设定两个显著性水平：

     * αE：进入模型的标准（通常设为 0.15，比 0.05 宽松）
     * αR：移除变量的标准（通常也设为 0.15）
   * 在选择过程中，如果一个变量在加入新变量后失去显著性（p 值 > αR），则需要移除。

3. **基于信息准则的方法**

   * 常用 **AIC** 或 **BIC**，特别是 BIC 会对模型复杂度（变量数量）有更大惩罚。

⚠️ 注意：不同方法未必得到相同的最终模型，也不能保证一定找到“最佳”子集模型。

---

**模型验证 (Model Validation)**

提出的问题：

* 我们能多大程度上信任回归模型？
* 如果用新数据测试，模型是否还能表现良好？

**方法**

1. **收集新数据来验证**

   * 最可靠，但通常昂贵、耗时或不可行。

2. **数据划分 (Data Splitting)**

   * 将原始数据分成 **训练集 (training set)** 和 **验证集 (validation set)**；
   * 训练集用于拟合模型，验证集用于评估模型。
   * 通常训练集 > 验证集。

3. **k 折交叉验证 (k-Fold Cross Validation)**

   * 将数据随机分为 k 个子集；
   * 每次用其中一个作为测试集，剩下 (k−1) 个作为训练集；
   * 重复 k 次，计算平均预测误差。
   * 常见评估指标：

     * **RMSE**（均方根误差）：越小越好；
     * **R²**（决定系数）：越接近 1 越好；
     * **MAE**（平均绝对误差）：越小越好。

**R 实现示例**

* 给定 Fertility ∼ Agriculture + Education + Catholic + InfantMortality
* 使用逐步回归、全子集搜索、交叉验证等方法得到模型；
* 输出最终拟合方程，如

  $$
  y = 2.0409 + 2.4546x_1 + 0.5447x_2 + 4.9945x_3 + 4.0033x_4
  $$

---

✅ **核心要点总结**

* **变量选择**：通过全搜索或逐步回归等方法确定最合适的预测变量组合。
* **逐步回归**：设定 αE 与 αR，逐步添加或删除变量。
* **模型验证**：通过新数据、数据划分或交叉验证来评估模型的预测能力。
* **评价指标**：RMSE、R²、MAE 是常用的预测性能评估标准。
 