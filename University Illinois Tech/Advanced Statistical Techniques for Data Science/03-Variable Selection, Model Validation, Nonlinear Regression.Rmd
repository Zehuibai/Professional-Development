---
title: "Variable Selection, Model Validation, Nonlinear Regression - Note"
author: Zehui Bai 
date: "2025-08-18"
output:
  html_document:
    df_print: paged
    number_sections: no
    toc: yes
    toc_float:
      collapsed: true
      smooth_scroll: true
    theme: journal # <!-- https://bootswatch.com/3/  -->
    highlight: tango
    code_folding: "hide" 
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

##《Logistic Regression》


**Logistic 回归 (第一部分)**

* **背景**：在之前的课程 (MATH 764 和 MATH 765) 中，因变量 $y$ 一直是连续的定量变量，预测变量可以是定量或定性。但实际上，因变量也可能是定性的（如二分类：有/没有）。
* **例子**：

  * 家庭保险：预测变量包括年龄、资产、职业，因变量是“是否有保险”。
  * 心脏病研究：预测变量包括年龄、性别、胆固醇、BMI、血压、吸烟史，因变量是“是否得心脏病”。
* **问题**：如果直接用线性回归拟合二分类概率：
  $\pi = β_0 + β_1x$，会出现概率 $\pi$ 不在 \[0,1] 区间的问题，并且误差方差随 $\pi$ 变化（异方差）。
* **解决方法**：引入**逻辑函数**（sigmoid，S 型曲线），保证预测概率在 (0,1)：

  $$
  \pi(x) = \frac{e^{β_0 + β_1x}}{1 + e^{β_0 + β_1x}}
  $$
* **推广**：多个预测变量时：

  $$
  \pi(x) = \frac{e^{β_0 + β_1x_1 + β_2x_2 + \cdots + β_px_p}}{1 + e^{β_0 + β_1x_1 + β_2x_2 + \cdots + β_px_p}}
  $$
* **Logit 转换**：

  $$
  \text{logit}(\pi) = \ln\left(\frac{\pi}{1-\pi}\right) = β_0 + β_1x_1 + \cdots + β_px_p
  $$

  这说明 logistic 回归实际上是用预测变量的**线性组合**去拟合对数几率（log-odds）。
* **Odds（几率）**：

  $$
  \text{odds} = \frac{\pi}{1-\pi} = \frac{P(\text{成功})}{P(\text{失败})}
  $$

  概率限制在 \[0,1]，但几率可以取大于 1 的值。例子：13 个球中 5 个白球 → 成功概率 = 5/13，几率 = 5:8。

---

**Logistic 回归 (第二部分)**

* **估计问题**：
  Logistic 回归没有闭式解，不能像线性回归那样用最小二乘。
  需要使用**极大似然估计 (MLE)** 来求参数 $\hat{β}$。
* **似然函数**：
  对于样本 $(x_i, y_i)$，有

  $$
  P(y_i|x_i) = \bigg(\frac{e^{β_0+β_1x_i}}{1+e^{β_0+β_1x_i}}\bigg)^{y_i} \cdot \bigg(\frac{1}{1+e^{β_0+β_1x_i}}\bigg)^{1-y_i}
  $$

  全部样本的联合概率是各个独立概率的乘积。取对数后得到对数似然函数，通过数值优化（如 Newton-Raphson）求最大化。
* **R 实现**：

  * `glm()` 函数 (family=binomial) 可以直接拟合 logistic 回归。
  * `predict(type="response")` 得到预测概率。
* **参数解释**：
  例：模型为

  $$
  \ln\left(\frac{\pi}{1-\pi}\right) = 1.0334 + 2.0313x
  $$

  这里 $\beta_1 = 2.0313$。
  解释：当 $x$ 增加 1 个单位时，odds 增加 $e^{2.0313} \approx 7.62$ 倍，即增加 662.4%。
* **多变量情况**：
  例：录取模型：

  $$
  \ln\left(\frac{\pi}{1-\pi}\right) = 0.13 + 0.0456X_1 + 0.032X_2
  $$

  * 数学成绩每增加 1 分，录取几率提高 4.67%。
  * 阅读成绩每增加 1 分，录取几率提高 3.25%。

---

👉 总结：

* Logistic 回归解决了因变量是二分类时，线性回归无法保证概率范围和方差稳定的问题。
* 用 logit 函数线性化几率，建立模型。
* 参数解释通常基于 **几率比 (odds ratio)**，便于说明变量对结果的影响。
* 参数估计依赖最大似然法，实际计算用软件完成。
 
 
##《Poisson Regression》 


**1. 背景与引入**

* 之前学过：

  * **连续型因变量** → 用 **线性回归**。
  * **二分类因变量 (0/1)** → 用 **逻辑回归**。
* 现在讨论：

  * **计数型因变量 (0, 1, 2, 3, …)** → 用 **Poisson 回归**。
* 举例：

  * 每年车祸死亡人数。
  * 某医院每月分娩数。
  * 战斗机执行 30 次任务中受到的损伤数。

---

**2. Poisson 分布回顾**

* 随机变量 $Y$ \~ Poisson(λ)，取值范围是 {0, 1, 2, …}。
* 概率质量函数：

  $$
  P(Y=k) = \frac{e^{-\lambda}\lambda^k}{k!}, \quad k = 0, 1, 2, \dots
  $$
* 参数 λ > 0，表示期望值 $E(Y)=λ$。

---

**3. Poisson 回归模型**

* 假设 $Y$ = 战机在 30 次任务中的受损次数。
* 期望值 $\lambda = E(Y)$ 可能取决于：

  * $X_1$：载弹量（定量）
  * $X_2$：机组飞行经验月份（定量）
  * $X_3$：白天/夜间任务（定性）
* 模型：

  $$
  \lambda(X) = e^{\beta_0 + \beta_1X_1 + \beta_2X_2 + \beta_3X_3}
  $$

  或者

  $$
  \ln(\lambda(X)) = \beta_0 + \beta_1X_1 + \beta_2X_2 + \beta_3X_3
  $$
* **为什么取对数 (log link)**？

  * 确保 λ 始终 ≥ 0。

---

**4. 参数估计**

* 用 **最大似然估计 (MLE)**。
* 假设观测值独立，则似然函数：

  $$
  L(\beta) = \prod_{i=1}^n \frac{e^{-\lambda(x_i)} \lambda(x_i)^{y_i}}{y_i!}, \quad 
  \lambda(x_i) = e^{\beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \beta_3 x_{i3}}
  $$
* 最大化这个似然函数来估计 $\beta$。

---

**5. R 实现**

* 使用 **glm()** 函数（广义线性模型）。

  ```R
  fit <- glm(Y ~ X, family=poisson, data=data)
  summary(fit)
  ```
* 预测时：

  ```R
  predict(fit, newdata, type="response")
  ```

  * `type="response"` → 输出的是 λ（均值）。
  * 默认输出的是线性预测值 $\beta_0+\beta_1X$。

---

**6. 参数解释**

* 示例模型：

  $$
  \ln(E(Y|X)) = 1.22914 - 0.04981X
  $$

  其中 $\hat{\beta}_1 = -0.04981$，p 值 = 0.686（不显著）。
* 参数含义：

  * 若 $X$ 增加 1 单位，则：

    $$
    \frac{\lambda_1 - \lambda_0}{\lambda_0} \times 100\% = (e^{\hat{\beta}_1}-1)\times 100\%
    $$
  * 在例子中：

    $$
    (e^{-0.04981} - 1) \times 100\% \approx -4.859\%
    $$

  → 平均事件数每增加 1 单位 $X$，期望值减少约 4.9%。

  * 但因为 **p 值不显著**，这个结果没有统计意义。

---

📌 **总结要点**

1. Poisson 回归用于建模 **计数型因变量**。
2. 通过 **对数连接函数 (log link)**，确保 λ > 0。
3. 参数估计使用 **最大似然估计**。
4. 参数解释：

   * $\beta_j$ 表示预测变量 $X_j$ 每增加 1 单位，期望计数按比例 $e^{\beta_j}$ 改变。
 

##《Generalized Linear Models》

 

**广义线性模型 (Generalized Linear Models, GLM)**

**1. 回归模型的统一框架**

* 在之前的学习中，我们已经接触了三类常见的回归模型：

  * **线性回归**：假设响应变量 $Y \sim N(\mu, \sigma^2)$。
  * **逻辑回归**：假设 $Y \sim Bernoulli(\pi)$。
  * **泊松回归**：假设 $Y \sim Poisson(\lambda)$。
* 它们有共同点：

  1. 都利用自变量 $X_1, \dots, X_p$ 来预测因变量 $Y$。
  2. 都假设 $Y$ 属于某个分布族（正态、伯努利、泊松等）。
  3. 都建模的是 **响应变量的均值**，并通过线性预测子（linear predictor）与解释变量建立联系。

---

**2. 链接函数 (Link Function)**

* 在线性回归中：
  $\mathbb{E}[Y|X] = \beta_0 + \beta_1X_1 + \dots + \beta_pX_p$
* 在逻辑回归中：
  $\log \frac{\mu}{1-\mu} = \beta_0 + \beta_1X_1 + \dots + \beta_pX_p$
* 在泊松回归中：
  $\log(\mu) = \beta_0 + \beta_1X_1 + \dots + \beta_pX_p$
* 一般地，我们用 **链接函数** $\eta(\mu)$ 把均值 $\mu = E[Y|X]$ 和线性预测子联系起来：

  $$
  \eta(\mu) = \beta_0 + \beta_1X_1 + \dots + \beta_pX_p
  $$

---

**3. 指数族分布 (Exponential Family)**

* 广义线性模型的理论基础是：响应变量 $Y$ 来自 **指数族分布**。
* 一般形式：

  $$
  f(x|\theta) = h(x) \exp\{\theta^T T(x) - A(\theta)\}
  $$
* 例子：

  * **伯努利分布**：$\theta = \log(\pi/(1-\pi))$
  * **泊松分布**：$\theta = \log \lambda$
  * **正态分布**：也可以写成指数族的形式。

---

**4. GLM 的一般定义**

* GLM 扩展了线性回归，允许：

  1. 响应变量 $Y$ 服从 **指数族分布**（Normal, Bernoulli, Poisson, Exponential, Gamma 等）。
  2. 响应变量的 **均值** 通过一个 **链接函数** 与线性预测子相关。
* 常见组合：

  | 响应分布             | 链接函数          | $\eta(\mu)$         |
  | ---------------- | ------------- | ------------------- |
  | 正态 (Normal)      | 恒等 (Identity) | $\mu$               |
  | 伯努利 (Bernoulli)  | logit         | $\log(\mu/(1-\mu))$ |
  | 泊松 (Poisson)     | log           | $\log(\mu)$         |
  | 指数 (Exponential) | inverse       | $-1/\mu$            |
  | Gamma            | inverse       | $-1/\mu$            |

---

**5. 在 R 中的实现**

* 使用函数 `glm()`：

  ```r
  glm(formula, family, ...)
  ```
* `family` 参数指定分布及链接函数，例如：

  * `binomial(link="logit")`
  * `gaussian(link="identity")`
  * `poisson(link="log")`
  * `gamma(link="inverse")`

---

✅ **总结要点**

* GLM 是对线性回归的推广。
* 通过 **指数族分布 + 链接函数** 统一了线性、逻辑、泊松等模型。
* 灵活性：可以选择不同的分布和链接函数来适应不同数据类型（连续、二元、计数、时间等）。
* 在统计软件（如 R）中，GLM 是建模的核心工具之一。
 

##《Robust Regression》

**1. 为什么需要稳健回归？**

* **普通最小二乘法 (OLS)** 的目标是最小化 **残差平方和**：

  $$
  S(\beta) = \sum_{i=1}^n \epsilon_i^2
  $$
* 由于对残差平方，会让 **离群点（outliers）获得过高权重**。
* 结果：一个离群点就可能把回归线“拉歪”。
* **稳健回归的目标**：在不删除数据的前提下，减小离群点的影响。

---

**2. M-估计 (M-estimation) 框架**

* 是 OLS 的推广。
* 将残差平方替换为一般的损失函数：

  $$
  S(\beta) = \sum_{i=1}^n H(\epsilon_i)
  $$

  其中 $H(\epsilon)$ 是更稳健的函数。

**对 $H(\epsilon)$ 的要求：**

* 非负；
* 在 $\epsilon=0$ 时取 0；
* 关于 0 对称；
* 随着 $|\epsilon|$ 单调递增；
* 可微分（方便推导）。

---

**3. 迭代加权最小二乘 (IRLS)**

* 对目标函数求导，得到估计方程。
* 可转化为一个 **加权最小二乘 (WLS)** 问题：

  $$
  w_i = w(\epsilon_i)
  $$
* 算法步骤：

  1. 用 OLS 做初始拟合；
  2. 计算残差；
  3. 根据残差更新权重；
  4. 用新权重做 WLS；
  5. 重复直到收敛。

这种方法叫 **IRLS（Iteratively Reweighted Least Squares，迭代加权最小二乘）**。

---

**4. 常见的稳健估计方法**

(a) **Huber M-估计量**

* 小残差时 → 像 OLS 一样二次惩罚；
* 大残差时 → 变成线性惩罚，减弱离群点影响。
* 阈值 $k \approx 1.345\sigma$。
* 权重函数：

  $$
  w_i = 
  \begin{cases}
  1 & |\epsilon_i| \leq k \\
  \frac{k}{|\epsilon_i|} & |\epsilon_i| > k
  \end{cases}
  $$
* **特点**：效率高（在正态分布下有约 95% 的 OLS 效率）。

(b) **Bisquare（双二次/Tukey）估计量**

* 小残差 → 二次形式；
* 大残差 → 权重逐渐减小到 0；
* 阈值 $k \approx 4.685\sigma$。
* 权重函数：

  $$
  w_i = \left(1 - \left(\frac{\epsilon_i}{k}\right)^2\right)^2 \quad (|\epsilon_i| < k), 
  \quad 0 \text{ 否则}
  $$
* **特点**：极端离群点最终完全没影响（权重=0）。

---


✅ **总结**：
稳健回归（通过 M-估计 + IRLS）在数据满足 OLS 假设时表现接近 OLS，但在有离群点时更稳定。常见的 Huber 和 Bisquare 方法在统计建模中广泛使用。
 
##《Variable Selection and Model Validation》

  

**变量选择 (Variable Selection)**

* 在实际研究中，通常会收集到很多潜在预测变量，但并不清楚哪些变量最重要。变量选择就是要从候选预测变量中找出最合适的子集，建立最优模型。
* 在临床试验等受控环境下，变量选择往往不必要，因为预测变量在设计时已确定。但在观察性研究中，由于收集的数据维度多、未知性强，变量选择就非常重要。

**自动化模型搜索方法**

1. **全子集回归 (Full Search)**

   * 若有 k 个预测变量，则可能的回归模型数为 2^k 个。
   * 例如 3 个预测变量 x1, x2, x3，共有 8 种模型（无预测变量、1 个预测变量、2 个预测变量、3 个预测变量）。
   * 然后用统计指标选择最佳模型。
   * 但当 k 较大时，计算量会指数爆炸。

2. **逐步回归 (Stepwise Regression)**

   * **前向选择 (Forward Selection)**：从空模型开始，每次根据显著性水准 αE 选择最显著的变量进入模型。若无变量满足条件，则停止。
   * **后向消除 (Backward Elimination)**：从全模型开始，每次移除不显著的变量（p 值大于 αR）。
   * 在前向选择过程中，每次加入新变量后，要重新检验已进入的变量，如果它们变得不显著（p > αR），则需要移除。
   * 通常软件的默认设置为 αE = 0.15，αR = 0.15。

**在 R 中的实现**

* 可以用 BIC（贝叶斯信息准则）作为选择标准，样本量越大，对额外变量的惩罚越重。
* 案例：使用 Fertility \~ Agriculture + Education + Catholic + InfantMortality 的数据，前向选择、后向消除、全子集回归得出相同的最终变量集，但不同方法不一定总一致。

---

**模型验证 (Model Validation)**

* 理想情况下，我们会收集新的独立数据集来检验模型的预测效果。
* 但在实际应用中，重新收集数据往往代价高昂或不可行。
* 因此，常用的方法是 **在已有数据中划分训练集和验证集**，在训练集上建模，并在验证集上检验预测性能。

 
---

1. **直接收集新数据验证**

   * 最有效，但往往成本高昂、耗时长，有时不可行。

2. **数据划分 (Data Splitting)**

   * 将现有数据划分为 **训练集 (Training Set)** 和 **验证集 (Validation Set)**。
   * 训练集用于拟合模型，验证集用于评估预测性能。
   * 常见做法：
     * **80% 数据** → 训练集（Training set），用于模型拟合。
     * **20% 数据** → 验证集（Test set），用于独立检验模型。
   * 这种划分要 **随机进行**，避免样本选择偏差。
   * 在 R 中，可以用 **`caret` 包** 的 `createDataPartition()` 来实现数据划分。

3. **k 折交叉验证 (k-Fold Cross Validation)**

* 单次划分可能带来偶然性，因此我们引入 **交叉验证**：
* **K 折交叉验证（K-fold Cross Validation）**：

  * 将训练集进一步分成 K 份（例如 K=5）。
  * 每次取其中 1 份作为验证集，其他 K-1 份作为训练集。
  * 重复 K 次，每次得到一个验证结果。
  * 最后综合这 K 次的结果，得到模型在训练数据上的平均表现。
* 这样做能更稳健地评估模型的泛化能力。

   * 将数据随机分为 k 个子集。
   * 每次选择 1 个子集作为验证集，其余 (k−1) 个子集作为训练集。
   * 重复 k 次，得到 k 个误差评估，最后取平均值。
   * 优点：更稳定，充分利用了数据。

**评估指标**

* **RMSE (均方根误差)**：预测值与真实值的平均差异，越小越好。
* **R² (判定系数)**：预测值与真实值的相关性，越大越好。
* **MAE (平均绝对误差)**：预测值与真实值的平均绝对差，越小越好。
 
 