---
title: 'University Johns Hopkins-Genomic Data Science Specialization'
author: "Zehui Bai"
date: '`r format(Sys.time())`'
output:
  html_document:
    df_print: paged
    number_sections: no
    toc: yes
    toc_float: yes
  word_document:
    toc: yes
  pdf_document:
    toc: yes
fontsize: 10pt
editor_options:
  chunk_output_type: console
colorlinks: yes
---

```{r setup, include=FALSE, echo = FALSE,message = FALSE, error = FALSE, warning = FALSE}
knitr::opts_chunk$set(echo = TRUE)

# <!-- ---------------------------------------------------------------------- -->
# <!--                    1. load the required packages                       -->
# <!-- ---------------------------------------------------------------------- --> 

## if(!require(psych)){install.packages("psych")}

packages<-c("tidyverse", "knitr")
ipak <- function(pkg){
  new.pkg <- pkg[!(pkg %in% installed.packages()[, "Package"])]
  if (length(new.pkg)) 
    install.packages(new.pkg, dependencies = TRUE)
  sapply(pkg, require, character.only = TRUE)
}
ipak(packages)
# <!-- ---------------------------------------------------------------------- --> 


# <!-- ---------------------------------------------------------------------- -->
# <!--                        2. Basic system settings                        -->
# <!-- ---------------------------------------------------------------------- -->
setwd(dirname(rstudioapi::getSourceEditorContext()$path))
getwd()
Sys.setlocale("LC_ALL","English")

## convert backslash to forward slash in R
# gsub('"', "", gsub("\\\\", "/", readClipboard()))

### get the path
# rstudioapi::getSourceEditorContext()$path
# dirname(rstudioapi::getSourceEditorContext()$path)

### set working directory
# getwd()
# setwd("c:/Users/zbai/Desktop")
# Sys.setlocale("LC_ALL","English")

### get the R Version
# paste(R.Version()[c("major", "minor")], collapse = ".")

### convert backslash to forward slash 
# scan("clipboard",what="string")
# gsub('"', "", gsub("\\\\", "/", readClipboard()))
# <!-- ---------------------------------------------------------------------- --> 



# <!-- ---------------------------------------------------------------------- -->
# <!--     3. Load the SASmarkdown package if the SAS output is required      -->
# <!-- ---------------------------------------------------------------------- -->
# library(SASmarkdown)
# ### Set SAS output
# ### Reset engine to R
# saspath <- "C:/SASHome/SASFoundation/9.4/sas.exe"
# sasopts <- "-nosplash -linesize 75"
# knitr::opts_chunk$set(engine="sashtml", engine.path=saspath,
#         engine.opts=sasopts, comment=NA)
# 
# # run these commands to convince yourself that
# # within this knitr session the engine changed.
# knitr::opts_chunk$get()$engine
# knitr::opts_chunk$get()$engine.path
# knitr::opts_chunk$get()$engine.opts
# <!-- ---------------------------------------------------------------------- -->



# <!-- ---------------------------------------------------------------------- -->
# <!--                         4. Import the datasets                         -->
# <!-- ---------------------------------------------------------------------- -->
### Import csv data
# pfad <- "~/Desktop/SASUniversityEdition/myfolders/Daten"
# mydata1 <- read.csv(file.path(pfad, "yourcsv_data.csv"), 
#                     sep=";", 
#                     header=TRUE)   

### Import xlsx data
# library(readxl)
# mydata2 <- read_excel("C:/Users/zbai/Documents/GitHub/R-Projects/SAS/Yimeng/results-text.xlsx")

### Import sas data
# library(sas7bdat)
# mydata3 <- read.sas7bdat("~/Desktop/SASUniversityEdition/myfolders/Daten/uis.sas7bdat")

### Import from copyboard
# copdat <- read.delim("clipboard")
# Data_D01 <- copdat

# <!-- ---------------------------------------------------------------------- -->
# <!--                           5. Some Tools                                -->
# <!-- ---------------------------------------------------------------------- -->

## To check out vignettes for one specific package
# browseVignettes("ggplot2")


# <!-- ---------------------------------------------------------------------- -->
```



```{r mind map,echo = F,message = FALSE, error = FALSE, warning = FALSE}
## Convert to mind map text, markdown outline, R script, and HTML widget ####
library(mindr)
# text -> widget
# input <- c("# Chapter 1", "## Section 1.1", "### Section 1.1.1", "## Section 1.2", "# Chapter 2")
# mm(from = input, root = "mindr")


input <- rstudioapi::getSourceEditorContext()$path 
## file.show(input) # Open the input file with the default program, if any
input_txt <- readLines(input, encoding = "UTF-8")
## Convert to mind map text, markdown outline, R script, and HTML widget ####
mm_output <- mm(input_txt, 
                output_type = c("widget"),
                root = "")
mm_output$widget
```

# Introduction to Genomic Technologies

## What Is Genomics

So the definition of, the dictionary definition of genomics, is on the slide that you're seeing. But we're not really going to use that. Genomics is descrip, it's the study of genomes. A genome is all of the molecular material inside your cells that defines how your body works and how our bodies work. There are multiple different things that we can study about genomes. When we talk about the structure, what we often mean is the actual sequence of nucleotides, As, Cs, Gs, and Ts in the genome. So a genome is a long, long molecule comprised of DNA nucleotides. And we call them, we use the letters A, C, G, and T to, as an abbreviation for these four nuclear ties. There's only four of them. In the human genome, there are approximately 3 billion of these letters strung together. The structure of the human genome is that it's divided into 23 chromosome pairs. 22 of them are, identical to one another is two, two nearly identical copies. One from mom and one from dad, and then you have x and y. And if you're a man, you have x and y. If you're a woman, you have x and x. So that's another way to think about the structure. And then within each chromosome, there's a little bit of more structure we occasionally talk about. In the middle of every chromosome, there's something called the centromere. And at the ends of the chromosomes, there are these special sequences that cap the chromosomes called telomere. So that's the structure of genome. But the function of a genome is, involves many, many other things. That's what we're, really when we sequence a genome, what we're really getting at, is trying to understand the function. So the function is all the things the genome does. So in that long, long string of letters, we have encoded everything that that your body can do. So we have the the instructions for how you make all the organs in your body. For how your body develops from a single embryo, into all these complicated tissues. And we also have instructions for how your body does things like respiration and metabolism. And even such complicated things as building a brain.

Another aspect of genomics is evolution. Evolution is a very broad topic. When we're talking about evolution of genomes, we talk about how genomes themselves change over time, and we usually mean over evolutionary time, and those are very, very long time periods. We know from having sequenced the human genomes, and now having sequenced many humans individual genomes that all of us are nearly identical to one another. So our genomes from generation to generation barely change at all and most of those changes are small, random changes which don't have any effect. But we can compare our genomes to genomes of other creatures such as chimpanzees which are our closest relatives. Diverge from us approximately 6 million years ago. And we can compare our genomes to genomes of much more distant things like fruit flies, nematodes, or even bacteria. And when we do that, we discover that, we share a surprising amount of sequence. Similarly, even with things as remote from us as bacteria. And when you think about it, that sort of makes sense because the, even though, we're, obviously, very different from bacteria, we do very, very different things, every living thing on the planet uses DNA as its basic code. Every living thing on the planet has to do certain things in common because of that. For example, every living thing has to copy its DNA in order to make copy of cells. So bacteria use a very similar mechanism to copy DNA as, as humans, so we find the genes in bacteria are, in those cases, similar to genes in humans. [SOUND] And then finally, when we talk about mapping genomes, we're actually beyond, mapping sometimes refers to just sequencing itself, capturing the sequence, but once we capture the sequence, the first thing we do after that is try to figure out where the genes are, and we'll talk a lot more about that later in the course. But, genes the word genes has, has changed a lot over the past 50 to 100 years. Today, we think about, one way to think about it is, is say an inhe, inheritable unit that is something that you can inherit from your parents. Another way to think about is, is a small section of the genome that encodes a protein which in turns does, has some function. And that's usually what we're talking about and genomics is, is the parts of the genome that encode proteins or that encode little bits of sequence that can turn into functional elements. [SOUND] So here's another definition of genomics that emphasizes not just the study of genomes themselves but also a little bit about for application. So why are we interested in genomes at all. There are many, many applications of genomics, today, and the, the list is growing, rapidly as, as we get better and better at sequencing, and as we discover more things that we can do with genomes. But it certainly includes medicine, many applications, in, in pharmacy, and if you're not looking at human genomes, agriculture and, and other areas of science. So genomics is a relatively new field of biology. But it's in some in, in many people's minds, it's part of biology. But let me just emphasize a few of the differences between genomics and more traditional biology and genetics, which have a big overlap. So one way you can distinguish genomics from more traditional biology. Genetics is the, traditionally when we, when we study genomes, or genes, we studied one at a time. Not that long ago, back in maybe the 1980s, it was a common, a common experiment was to, to study one gene and to spend months or years studying that one gene, or people would spend their whole careers. You've been studying one gene and writing about that gene, and trying to figure out what that, what function, what the function of that gene was in said people or in say, a model version like mouse. But today, because we can capture the whole genome, we often look at all the genes at once or at large collections of genes all at once, so that would be a more genomics way of studying. The technology has been the real driver of this, so that's a very big differences and that's really what's allowed genomics to grow the way it has the past 20 years. In the past, we could only do targeted, what we call low throughput experiments, studying one gene at a time instead of one organism. Today, we can do experiments where we simultaneously measure the activity of thousands of genes all at once using the new genomics technology. So that's a big difference between genomics and earlier ways of doing biology. And then the hard part, where things differ between traditional and genomics science. In the past, you still had to be clever, you had to understand biology and genetics well. Experiments were not necessarily easy to do. Might, they might take years. Today, some of the experiments, you can still do the same kind of experiments, some of the experiments are easier to do, but we generate so much data that the data itself is overwhelming. So, because the technology has gotten very clever, very efficient, and very expensive, we're now looking at doing experiments where we, where we measure thousands or tens of thousands of genes all at once, in multiple tissues, in multiple samples. And we know going into such experiments that there's probably a lot of really interesting results that are going to come out. But when we get the data, we discover that, oh my god, there's so much data here that it's hard to figure out where the interesting results are. So we have, we have to deal with big data problems that we didn't have to deal with before. We have to deal with, statistical uncertainty in ways that we didn't have to deal before. And we have to deal with large-scale computation in ways that were never necessary for earlier, earlier types of biological studies.

## Genomic Data Science

So, this lecture is about what genomic data science is. So, I'm going to try to quickly introduce the major disciplines that comprise genomic data science and tell you what, what we consider to be part of it. So, genomic data science is at the intersection of biology, statistics and computer science. And we're calling it genomic data science. But it actually goes by many other names, some of which you might have heard, such as computational genomics, computational biology, bioinformatics, or statistical genomics. Regardless of which name you use, the activities of genomic data scientists do all sort of fall into these same and to similar categories. So what do we do in, in genomic data science? We start with some, some, let's talk about sort of humans, we start with some subjects. By the way, it doesn't have to be humans, it could be mice or model organisms. But let's say we're talking about humans. We'd want to go and collect some samples from those humans. So, those could just be, if we're looking at normal development, we might just collect some skin cells from normal people. And then prepare those samples in the lab, send them for sequencing. The sequencer then generates enormous amounts of data that we have to do something with. So what we typically will do when we're looking at humans is, we take those sequences which are very short fragments. We call those reads, because you're reading the DNA, so I'll use the term reads, and others will in this course as well. To mean a little bit of sequences come off a sequencer which is part of someone's genome. And we take those reads and we align them to the reference human genome, and there is one reference genome, for now. Which represents, for now, sort of average northern European male. Oh, in the near future we're probably going to have many other reference genomes as well. But in any case, we align these reads to the reference genome and that tells us how the differ from the reference genome. And also lets us kind of compile them all up on one another and see how, see what sort of differences there are even within the genome. Now, remember for every genome of a person you actually have two copies of every chromosome. And that's because you got one copy from mom and one from dad. So one of the things we study for example at this phase, is how do your copies of those, of each gene differ between the two copies that you have. And then once we've done some kind of analysis we may deposit that data in a public database such as one of the databases at N, at NCBI. And we apply many other types of analyses to these data sets to make more biological conclusions about what's going on. So that's sort of very, very broadly speaking, what genomic data science is. So we start typically with experimental design. So if you're, if you're going to do genomics, you have to first think about, well, I have a question, a scientific question I want to answer. And I have to decide, well, how much data do I need? How many subjects do I need? What kind of data we get? I've been talking about sequence data so far, but actually there's various modifications we can do. Some new technologies that go by names like ChIP-seq and methyl-seq. They'll let you study other things about the genome as well. So the first thing you do is you, you come up with an experimental design which if everything goes will you hope will allow you to answer those questions. And that's really critical. If you don't, these experiments, even though we're generating a lot of data far more cheaply than before, it's still is expensive and time consuming to do these experiments. And you don't want to reach the end of your experiment and discover that you simply don't have the right data to answer your question. So once you've decided on your experimental design, you generate your data, these, then your typical, then you'll take those big data sets and as I was saying a few minutes ago. The first thing you would typically do is align them to the reference genome and find out how they're, how they differ from that genome and assemble them together in some form. So that might, that might mean that you're looking at say, if you've captured RNA from the, from the cells. When you align them to the genome you can see, for every gene in the genome, how much of that gene was present. So, that's the kind of thing that you would, you'd be doing at the pa, part where you're aligning and assembling things. You might be assembling the genes that were expressed in a set of cells or tissue you were studying. So, another big s, important step, in this process is preprocessing and normalization of the data. Now these, because these data sets are very big, there's, there's various types of, of biases that can, that can come out. And this is sort of the, the law of large numbers comes into play a lot here. Even, all of the technology that we're talking about, even though I'm talking about it like it's very nice, clean technology, it's not perfect. So the sequencing machine itself, it makes mistakes. The process of collecting the data introduces biases. We might collect more of some tissues than others. We might collect some genes might be representative of higher levels because of biases, not because of true bio, biological differences. The sequences machines themselves sometimes have have errors that are not random and are hard to identify. And sometimes they have other errors that are random and we can identify those and get rid of those. So we basically want to take these big data sets and apply some computational and statistical methods. To, to sort of correct as best we can any kinds of systematic errors or any sort of bias that are in that data before we move onto the next steps. So once we've got the data normalized, preprocessed and normalized we then apply a variety of techniques. That come from fields within statistics, within computer science and within biology to, to make our conclusions. So, so for example, in statistics and machine learning scientists develop a wide variety of techniques, some of which we'll talk about later in this course. To, to go from this preprocessed normalized data to scientific conclusions.

So, in order to then take that normalized data and make this conclusions one of the things that people within the field of genomic data science themselves do is develop software. So when we've, once we've, when we have an experiment and I'll just take one as an example. One that I'm, I'm, very familiar with is RNAC. because RNAC, which we'll talk about probably time and again in this course, is, is a set of experiments or an experimental protocol. Where you can capture just the genes that are being turned on in a set of cells. And you measure those by sequencing. And this is a pro, this is an experimental, an experimental paradome. It's very popular. You can use it to study cancer. You can use, use it to study development. You can use it to study evolution and hundreds or thousands of different kinds of conditions. And because of that, the experimental protocol for doing the RNAC part is more or less standardized. That means that we can design software that will in a more or less standardized way go from this raw sequence data to something which is more amenable to making biological conclusions. What you really want when you're doing RNA seq-, RNA sequencing is you would like to know okay, I had my samples. I want to know in each sample what genes were turned on and what levels were those genes present in? So there are, there are software packages to do that, are developed by people in, in genomic data science. And software development of course requires, not just getting the code to work one time. But also documenting it, making sure it works for lots of different cases. So there's a lot of software engineering kind of principles, and, and algorithmic principles that are important in doing that sort of development. So another thing that we study in genome data science is broa, another broad category of, of questions, or population genomics. So, I've mostly been talking about individual sorts of questions like what causes a cancer to be a cancer? But we can study other things too, such as we can look at a population and ask, what makes a particular group of people more susceptible to a particular type of disease? Or why do some people have, resistance to a particular type of disease? So, or why do some people have certain traits in their, in their population? So population genomics refers to looking at, rather than looking at one person at a time or small groups, look at whole populations and fi, and, and, and studying their genomes to see how they differ. And how those differences lead to, to recognizable differences in, in the people in those populations.

So, so another way to, to characterize genomics and to characterize some work in, in genomic data science is, when we collect experiments of different types and pull them all together. So you might call that integrative genomics, you could also call it systems biology, another popular term. But that refers to computational and statistical activities where we're taking data that comes from sequencing experiments, say of different types. As well as other kinds of measurements. They might be measurements on what we call the proteon, measurements separate from the DNA. And integrating this all together to make biological conclusions. So we would call that integrative genomics.
 
 
## Applications of Sequencing

So, in this lecture, we're going to talk about next generation sequencing applications. The introduction of next generation sequencing technology, which has made sequencing so fast and so cheap, has allowed scientists to come up with all sorts of creative new types of experiments that they can use sequencing to do. So, another way to think about it is that we can now ask scientific questions and answer them with sequencing. Questions, that we've, we've had for decades in many cases, but sequencing was simply too expensive or too slow to answer them before. Well, today through next generation sequencing applications, and through some clever new experimental methodologies, we can answer all kinds of interesting questions using sequencing alone. So let's, let's look at some of those some of those methods right now. So the basic idea is we need to create DNA, because if we're going to sequence it, we need DNA. To convert a molecule into DNA, we might start DNA and just sequence it or we might start with RNA and convert it to DNA and sequence it. And then apply second generation sequencing to measure something. So, let's look at a few applications like this.
 
So, one very, very popular application today is exome sequencing. So, whats the exome? The exome is the collection of all the exons in your genome. So, what are exons? We've talked about that in other lectures, but let me just quickly review. Your DNA gets transcribed into RNA. And the RNA then gets chopped up into exons and introns. The introns get thrown away, the exons that remain are concatenated together, and those exons then get translated into proteins. So if you want to know what the proteins are that are being turned on in a cell that are, that are in a, in a collection of cells, you need to know what the exons are. So, in particular, in the world of genetics, when we're looking for genetic mutations, we're mostly, we're mostly interested, or usually interested first in, in mutations that affect proteins. So, those mutations should occur in exons. So, we can capture just the exons in a cell and sequence those. And you might say, well, why would we do that, we can sequence the whole genome? Well the exons only comprise about 1.5% of your genome. So you can sequence much less DNA and still get a picture of your entire exon, your entire exome. So how do we do that? We take the DNA molecule, so only some parts of the DNA like I said about a 1.5% of your genome or maybe on the order of 30 to 60 million bases will be captured as exons. And there are kits that do this. They will, they will capture this for you. So we want to take that, that protein coding exon, you want to fragment your DNA, the whole genome, whole genomic DNA from a person whose exome we're sequencing and some of that will be exons and fragments of exons and some of it won't. And we want to just capture the exons. And so the kits that have been developed are kits where you have a a bead, a magnetic bead, typically. And on that bead you'll have pieces of DNA that are only found in exons attached, and this is single stranded DNA. When you're preparing your DNA for sequencing you make it single stranded by heating it up a little bit. And then the, the DNA that belongs to exons will hybridize to the complementary DNA that's attached to those chips. And then you can pull those chips down, and then remove the active of DNA attached to them, and sequence it. And that way you just have sequenced he exons. So that's exome sequencing, you only sequence the exonic parts of DNA, and kids today will capture around the order of 50 to 60 million base pairs out of a person's genome when they're doing exome sequencing.

Another technology is RNA-seq, or RNA sequencing. So, this, this involves trying to capture all the genes that are being turned on in a cell, or in a collection of cells. So as I just said, to, to produce a protein, DNA first gets transcribed into RNA, and then translated into proteins. So if we can capture the RNA, that gives us a picture of which genes are being expressed or turned on, in a particular cells, set of cells or cell type. So a very important feature of the RNA molecule, is that after transcription, the cell attaches a long string of A's to it. So we can use that, and that's sort of the basis of RNA-seq technology is that all the molecules that we're interested in have these long stretches of A's on the end. Anything that doesn't have a long stretch of A's we can ignore.

So we capture that poly A tail by various techniques. Basically, we use a string of T's that we know will stick to all those A's. And we, we, attach those T's to something we can grab a hold of, and through that we capture the mature mRNA by it's poly A tail.

And once we've done that, we would have to then, we can't sequence RNA. We have to turn it into DNA. So fortunately we have a very, a very useful molecular mechanism, invented by evolution that, that does reverse transcription. So, there's, there's, the number of virus that do this as their way of, of surviving. So we have reverse transcriptase, and there's a number of different reverse transcriptase molecules we can use that will take RNA and copy it back into DNA. So rather than going from DNA to RNA, you can actually go from RNA to DNA using this special molecule called reverse transcriptase, and that gives us the DNA that matches the RNA that, that is that we've just captured. Once we have DNA, we just sequence it. And from then, from that point on, it's a, it's a computational problem to figure out which cells, or which genes would turn on those cells. A very complicated computational problem, but it's important in trying to solve that problem that you realize where this data came from.

A third technology that's become very popular through since Next Generation Sequencing Technology was introduced is ChIP-Seq. So ChIP-Seq is trying to address a different problem. Which is trying, which is the problem of understanding where on the DNA certain proteins might bind. Now the way that DNA controls gene expression, the way that our cells control gene expression so that cells can behave differently from one another, is that some genes are, are inhibited in certain cell types or or enhanced in certain cell types. And the way that happens is that you have transcription factors that is other genes themselves, proteins themselves, that bind to the DNA and control the expression of the genes that are near the place where the protein is binding. And we'd like to know where that's happening. Now of course we don't have today, microscopes that'll let us just look at the, at the chromosome and see where there are proteins bound to it, but we can do something indirect, again, using sequencing. We can, we can, link the proteins, basically freeze the protein right onto the DNA through a process called cross-linking. So we can take a set of cells that we're interested in, a particular cell type. We can cross-link the proteins to the DNA in those cells, so now the protein is basically stuck there. We want to know okay well where was it stuck?

So what we can do is we can then take that DNA where the proteins are stuck, we can fragment it lots, millions and millions of fragments. Most of them do not have any protein stuck to them, but some of them these are sharp fragments, will have protein stuck to them. We can then grab these proteins, we've, we've, we've designed in ChIP-Seq we've designed antibodies that will, that will pull those proteins out of the mixture. So it can pull those proteins out. And when we pull them out, because they're frozen to the DNA we'll pull out these little fragments of DNA that those proteins were bound to at the same time. We can then remove the protein and sequence the DNA. And again, we've turned, now we've turned our problem into a sequencing problem. The sequences that come out are short fragments and we know because of the way we do the experiment that those fragments were protein binding sites for whatever we were, we were targeting with that antibody.

Then finally let me talk about one more technique which is called bisulfate sequencing or methelsync. This is a way of determining where on the, the genome the DNA has been methylated. So methylation is this important epigenetic modification that also affects which proteins are, are being expressed in the cell. And this methylation, methylation marks, or methyl groups, can be passed on from one cell cycle to another as cells divide. So how do we figure out where the DNA was methylated? Well, one way to do this experiment is to split your DNA into two identical samples. You take two very small samples, or aliquots of DNA. And then you treat one of them in a special way, doing something called bisulfite conversion. And this biochemical process converts all of the C's that are not methylated to U's. Oh, one thing I didn't mention was that the methyl groups are always attached to C's, that's the only, that's the only new they get attached to. So this process now gives us, so now we have two identical samples, where we've converted in one sample, we've converted the, the, the DNA in a special way, so that many of the C's are now U's. And then, we sequence again, and we have to compare those, and this requires very specialized programs that can do not just the comparison but also the alignment because that, that converted DNA now doesn't really match the genome very well, the reference genome that we usually align to. So we need to use a special aligner that allows these, these U's to now match what would have been C's in the original genome. So that's methyl seek, It's a way of measuring methylation on a, on a set of cells, of, or tissues.

## Memory and Data Structures

This lecture is about how we store data, data structures and computer memory. Data structures are an important part of computer science that lets us store data efficiently, and especially when we're dealing with very large data sets, we have to think carefully about how we store data. So, for example, in the context of genomics, we, we deal with large amounts of sequence data, sometimes aligned, as you can see on this slide. And we have to figure out, all right, we have sequences for many people or many species. We might have many sequences from all these people. We have to think about how to get those into memory. Now the way the sequences come at us from a sequencing machine is just as a long string of characters of text. And computers are very good at storing text, that's what they were originally designed to do in many cases, so computers already store, have a way to store that. But sometimes we can think about more efficient ways to store that. So for example, when we're looking at a multiple alignment of lots of sequences, there are lots of things those sequences have in common we might want to think about storing storing something that's a little bit smaller than all the separate sequences and just storing the differences between them. Another important aspect of data structures and memory, when we're doing, when we're designing them, is that we want to be able to find stuff. We're looking at, when we're looking at DNA sequences, they all pretty much look alike. They're a long string of As, Cs, Gs, and Ts. And when we're looking at the human genome, we have three billion base pairs to search through. So for example, an interesting problem to think about in the data structure point of, from the data structure point of view in genomics is, well I've got a little sequence of, say 100 nucleotides, 100 DNA letters, and I want to be able to store in such a way that I can go and quickly find it again. And I'm storing it in the context of a three billion base pair sequence. So, rather than just throw it in some random, like throwing it in a random pile, I'd like to store it in a way where I can go back and quickly find it. So, for example, with a DNA sequence that's from the human genome we might want to store some kind of tag that says what chromosome it's on, and what location it starts, where does it start at, in that chromosome.

So, the kinds of data structures that computer scientists have designed over the years vary widely, but one of the most common sorts of data structures is a, is a tree. There's also something called a list and something called a link list, and many variants on, on these data structures. And these are simply ways of keeping track of things, keeping track of objects you stored, and having objects point to one another, so that once you go to one object you can quickly find another object. So and to, to understand how these objects work, it's important to understand that in computer memory, we not only have the data itself, but we also have an address. So every piece of memory has an address, and we can find any, any object in memory if we know that address. So those addresses are called, in programming language terms, are called pointers and those pointers will take us directly to a piece of memory. So, if we store a piece of sequence data somewhere in the computer's memory, to retrieve that again, we simply need a pointer. And one way to think about a data structure that's efficient is if we have lots of sequences that are in, that are near each other in the genome, we might want to store pointers from one seq, one of those sequences to the next one so that once we're in the right place we can quickly find these other sequences without having to start over again from scratch.

So another aspect of data structures, thinking about data structures is to make, is making them efficient. As I said before, in the genomics world we're mostly dealing with sequence data. Sequence data has a natural representation as letters and computers represent letters typically as as one byte. So any, any letter in the alphabet, a through z, any nu, any numeral zero through nine is represented the same way inside the computer using one byte. So a byte actually of, of the word byte comes from the word bit. A bit is a binary digit. And that's just a zero or a one, and that's at the most fundamental level how computers represent information. If you take eight bits in a row, you can con-, you can consider that as an eight bit binary number, which, which can store up to 128 values. Usually we'd consider those to be the values 0 to 127. And the standard representation of text in the, in the, inside the computer is to represent every letter as one of those values between 0 and 127. So with that much space to, to represent information, we can represent all the lower case letters, all the upper case letters, that's another 26. We could represent, represent the ten single digits, that's another ten, and then we have a room for all the special characters. So basically everything on your computer keyboard is represented as a single byte. However, if you look at DNA, you see right away, well, there's actually only four letters there. So we can do much, much better when we're representing DNA. And this is how most serious, highly efficient programs for processing lots of DNA operate internally. Instead of representing the four DNA letters as one byte each, we can represent them as just two bits. So simply take A and call that, make that, represent that by the the two bits 0 0, C is 01, G is 1 0 and T is 1 1. And by doing it this way, we get a fourfold compression. So instead of using eight bits per letter of DNA, we're only using two bits. So, because we're storing gigabytes or even terabytes of DNA sequence data, a four-fold compression right out, right out of the box, is, is an important efficiency we can gain by that, that representation. So, finally to look at a slightly more sophisticated way of representing representing DNA when we're talking about the application of DNA, one thing that we like to capture in, in analyzing DNA are patterns of sequence that have some biological function. And here I'm showing you a picture of the, the ends of an intron. So introns are the, the interrupting sequences that are in the genes in our genome that actually don't encode proteins, but get snipped out and thrown away in the process of going from DNA to RNA to protein. And introns almost always start with the letters GT and they almost always end with the letters AG. And if you collect lots of them together and, and notice how these patterns are in common you can get the, you can create a probabilistic picture of what letters are most likely to occur at the beginnings and ends of introns. So these two pictures show you exactly those two pictures for the beginning of an intron which is called the donor site and the end which is called an acceptor site. So now we could represent all the donor sites we've ever seen as a big set of strings of say ten letters long, if we, if we chopped out a window of ten bases around those sites, or we could be much more efficient about it and capture much more interesting data by computing for every position in that little window the probability that the letter was A, C, G, or T. And these logos you see across the top use use the height of the letter to represent the probability that letter appears at that location. And with this kind of representation we've now compressed, essentially compressed, the information from hundreds or even thousands of sequences that we've seen into a simple pattern which we can then use to, to process other data to, for, for example to recognize these patterns when we see them again.

## Data Sharing Plans

One of the main issues in the original motivating example I gave you was that the data weren't available. They couldn't be analyzed by other people. And so it's important to have a data sharing plan. This is an important component of the statistical analysis of any genomic data set. And so, a data set consists of four actual components. First is the raw data. In the case of sequencing data, this is often the raw sequencing reads. That might be something like a FASTQ file or in a line file, a BAM file of line reads. Then there's a tidy data set. The tidy data set, we'll talk about in just a minute, is a data set where you've actually done some processing and cleaning so the data is easily analyzable and easily made interactive. Then you have to have a code book. This code book describes each variable and its values in the tidy data set. And finally, you need an explicit and exact recipe you use to go from the raw data to the tiny data set and the code book. Without all of these four parts, a data set is incomplete when you're sharing it.

## Plotting Your Data

Most statistical analysis of genomic data should be done interactively. And what we usually mean by that is by using plots. Another way of saying this is that you should take big data and make it as small as possible, as quickly as possible. This is a quote attributed to Robert Gentleman at Genentech. And what he was trying to say is that if you have a gigantic data set, say, made of millions or billions of reads, it's very hard to visualize what's going on with those data or understand the different properties or the characteristics of those data. And so the idea is you want to summarize them down just enough that you're able to plot them and visualize them and try to figure out what's going on.

And it's really important to do interactive analysis with lots of plotting, because sometimes the summary measures, the statistical summary measures that are often reported, say for associations can be a little bit deceptive. As an example of that, here are four plots. Each plot corresponds to a different data set. Each of these data sets seems statistically identical if you look at the coefficient for the slope, the p-value for the slope, or the correlation coefficient. So all of these data sets from the perspective of statistical summary measures are identical. But as you can see, there seem to be very different patterns going on in these different data sets. For example, the pattern in the upper right-hand corner looks a bit like sort of a quadratic shape. The pattern in the lower left-hand corner has a clear outlier, and so does the panel in the lower right-hand corner in a totally different direction.

And so the idea is the thing that you want to be doing is taking your summaries of your raw data and plotting them as quickly as possible, so you can try to identify characteristics or features of the data that might be important.

So interactivity allows for more discovery. And so one thing that you want to do when doing an interactive analysis, where you're trying to discover what's going on, is show as much of the data as you can in your plots. So here's an example. On the left is what I would call a bad plot. And lots of people, particularly statisticians, don't like plots like this because it shows bar plots with confidence bounds. But those bar plots don't show any of the actual raw data. So it's very hard to know if there was an outlier driving any of the analysis or how many data points were even used to create these bar plots. On the other hand, on the right-hand side, there's a much better plot. It shows the same information as the bar plot in terms of showing the average value in the confidence bounds, but it also shows the raw data. So you can actually see when there's differences in distributions. And in this case, there's only three points in each group. So you might have a little bit less faith in any conclusion you might draw than you might get from using the bad plot, where you don't actually know how much data is in the plot.

Another thing that you might do is plot replicates. This is a very common plot that you would do when you're doing an interactive analysis. So here you might want to compare, say, you ran the same sample through the technology twice, technical replicates. And you want to see if those two replicates produce similar results. So here's a plot on x, the x-axis is replicate one, and on the y-axis is replicate two. And so here, they look very, very correlated. So that's very good, you might see this and be comforted or think, okay, this technology is doing very well. There's a couple of tricky things, though, especially about plotting replicates. So the first thing is be careful of scale. So if you go back to this plot, 99% of the data is in the tiny little lower left-hand corner that I've shown below to, below and to the left of the light blue line here. So what you're seeing, just sort of the 1% of the data that ends up getting spread out. So one way that you can deal with this sort of tightly clustered data, particularly for replicates, is to use transforms. One example of a data transform is the log transform. So if you take the log of the data and then make the same plot, you can see they're correlated. But now the data is much more spread out and all of the data that was super tightly clustered down in the lower left-hand corner has been spread out a little bit. You get a little bit better idea about what's going on in the plot.

Another thing that people typically do when comparing two samples or two replicates is instead of plotting one replicate versus the other, they make something that's called a Bland-Altman plot. In genomics, this is often called an MA plot, and this is one of the most common plots and widely used in a variety of different technologies. So the idea here is rather than plotting replicate one on the x-axis and replicate two on the y-axis, you add the two replicates on the x-axis and you subtract the two re, replicates on the y-axis. So what does this mean? Moving from left to right are the, in this case, it's gene, each dot represents a gene. On the left-hand side are the genes that are very lowly expressed. And on the right-hand side are the genes that are very highly expressed. Then if you look at the y-axis, how far you are from zero is how different the two replicates are from each other. And so what you would like to see is all the points lining exactly on the zero line, which obviously never happens in real life. But what you can see here is, for example, a trend. There seem to be more differences between the replicates for the lowly expressed genes. This is a very common phenomenon. And so it's better to see that when you make the MA style plot as opposed to making the plot of one replicate versus the other.

One thing that you should be very careful of, and is a very common problem in genomics, is what has been coined ridiculograms. So a ridiulogram is defined as, in general, it can be something a little bit more general than this. But it has been defined as a network plot that looks beautiful but communicates very little information and appears on the cover of Science and Nature. And so what these plots are, you often see these sort of hairball-type plots. There are other plots just like them that don't actually communicate a message. They just kind of look beautiful. And so the point of statistical graphics is both to look pretty and to be you know, enjoyable to look at, but more importantly, to communicate scientific information to the reader. So bewaring ridiculograms is an important component of making sure your plots are interpretable.

## Three variability

**Var (Genomic Measurement)**

1. Phenotypic Variability
2. Measurement error
3. Natureal Biological Variation

So, variability of a genomic measurement can be broken down into three types, the phenotypic variability. So, imagine you're doing a comparison between cancers and controls. Then there's variability between the cancer patients and the control patients about their genomic measurements. So this is often the variability that we care about, we want to detect differences between groups. There's also measurement error, all genomic technologies measure whether it's gene expression, methylation, whether it's the alleles that we measure in a DNA study. All of those are measured with error and so we have to take into account how well does the machi, machine actually measure the reads, how long we quantify the reads and so forth. There's also a component of variation that often gets ignored or missed which is natural biological variation.
Play video starting at :5:55 and follow transcript5:55
So for every kind of genomic measurement that we take, there's natural variation between people. So even if you have two people that are healthy, have the same phenotypes in every possible way, they're the same sex, the same age, they eat in the same breakfast. There is still going to be variation between people and that natural biological variability has to be accounted for when performing statistical modeling as well. An important consideration is that there's often a rush when there's new technologies to sort of claim that this new technology is so much better than the previous technology. One way they do that is by saying that the variability is much lower that may be true for the technical component or the measurement error component of variability, but it doesn't eliminate biological variability. 

## Study Design, Batch Effects, and Confounding

Another important component of study design and experimental design are confounding and batch effects.

So, what is confounding? I'm going to give you an example using a very simple data set. So, this is a picture of me and my son. So, my son is three years old, he has small shoes and he's not very literate yet. I have bigger shoes, and I guess you could say I am somewhat literate. So using this data set, we might conclude that shoe size is associated with literacy. But, is that really true? Do we really believe that small shoes equals low literacy and big shoes equals big literacy? The reason why we might not believe this is because there's actually one piece of data that we hadn't concluded in our analysis. My son is relatively young, and I'm middle aged. And it turns out that age is more closely causally related to literacy. So if you make a plot the, the relationship between shoe size, literacy, and age, you see that age is related to shoe size. When you're young you have small shoes, and when you're old you have la, bigger shoes. And it's also related to literacy. When you're young, you're not very literate, and when you're older, you become more literate. And so, this, this variable that's related to both shoe size and literacy is what's called a confounder. So the confounder is a variable that's related to two other variables, and may potentially make it look like there's a relationship between those variables, even when there isn't. So, this is actually a very common problem is genomics and the most common confounder, the one that trips up the most people, is what's called batch effects. And so, here's and example of a batch effect. So this is a paper whe, that was originally published that looked for differences in gene expressions between ethnic groups. So he identified that 78% of genes were differentially expressed between the two ethnic groups. So you can see that in the p value histogram on the lower left. There are tons of tiny p values. So that's lots genes that look like they're differentially expressed between the two groups. This seems like really important and big result because there's very little actually genetic variation between different ethnic groups. So, there, it's surprising that almost all of the genes are differentially expressed. So turns out if you go back and look at when the data were collected all of the samples that come from Europeans were collected in 2003, 2004 and 2005. Whereas all of the samples that come from Asians, were collected later, were collected in 2006. So it turns out there's just enough overlap that you can kind of distinguish between the results that are, are, the genes that are different because of the date and the genes that are different because of the population. So if you look at the between population differences, it looks like 78% of genes are differentially expressed. If you look at the differences between the years when the samples were taken, 96% of the genes are differentially expressed. And once you adjust for the fact that the samples were taken in different years, all the difference between the population goes away. So this is what's called a batch affect. it basically suggests that there's a confounder, which is the date that the samples were taken. Why would the date matter? Well the technology might change, the assays might change, the aliquot that they take might change, or maybe the freezer broke in between the two samples. There are a number of reasons that the date might be associated with differential expression, and in fact in almost every study, this is a major effect. So it's not true just in the gene expression studies, it's also true in genetic study. So this is another big picture of genetic studies that looked for relationships between SNPS, single nucleotide polymorphisms and human longevity. And so they looked and, and saw that, they claim there was a small set of genes that would predict whether you would live to be 100 or not. But it turns out they measured all the younger people with one technology and all the older people with another technology and this study was subsequently retracted. Similarly, there's a example of protiomics where a predictor was developed based on protiomic patterns to predict ovarian cancer or or in this case it also fell apart largely because of study design. The ovarian cancer patients were sampled at a different time than the healthy patients were sampled. And so it was impossible to distinguish whether it was due to the confounderate batch or whether it was due, to the actual difference in biology that we care about.

This ends up being a huge problem, and it affects many technologies. And so, this is a paper where there's a discussion of how batch effects impact almost every genomic measurement. How do we deal with these potential confounders? One way is randomization. So imagine for example that we are trying to do a comparison here. And, so without randomization, th, this is what you might see. So, what I have is experimental units where the samples are shown as circles. And, so the treatments they might be given are the red and green circles around those samples, on th, right hand column. So suppose that there's another confounding variable. So it might be the age or the date or whatever variable you might consider. And so here in this case, the date or the age is related to the treatment. In other words the darker circles are more often get the green treatment, and the lighter circles more often take the red treatment.

So one way that you could address this is by simply randomly assigning treatments. So every new patient comes in and you assign them to either re, red or green and you do it with the toss of a coin. This will break down the relationship between the treatment and the confounding variable. And since it's random, it will actually break down the relationship, regardless of the what the other confounding variable is. So randomization is one way to address the potential problem of confounding. Another example is through stratification. So, in addition to randomization, you can actually design your experiment around confounders that you may have already heard of and know about. So here's an example. It's a study in mice. There are 20 males and 20 females. Half are going to be treated, and the other half will be left untreated. And you can only perform this experiment on four samples per day, four mice per day. So the question might be, how do you assign individuals to treatment groups and to days?

So a bad design would be to go ahead and basically do, you have the treated and the controls. Run all the controls as only females in the first week, and all the treateds to be only males in the second week. Here, you have all sorts of confounding. You don't know whether the treatment and the, so the treatment and the control are related to the data, the samples that are collected. They're also related to the sex of the mice, and so you, there's a very difficult ability to separate out the different sources of signal.

A stratified sample, on the other hand, might do something like this. So you would run both treated and controlls in both week one and week two. You would make some of the treated be males and some of the treated be females. And some of the treated be control sorry, some of the controls be males and females and run in week one and week two. So when we balance out the variables in this way, since we knew the potential confounders, the date and the sex of the mice, we were able to sort of design the experiment around these confounders. And so we can estimate their effects independently of each other. So these are some other good study characteristics, and in fact there's a long class that can be given on experimental design. We'll talk a little bit more about it in the statistics class. But just to give you an idea of a couple of other things that are important for doing good experimental design. In general, it's better to have a balance design. In other words, if you're going to do treated and controlled, you should have about equal numbers of treated and control samples. A same, a study should be replicated. In other words, if you only take one sample from one person, you have no idea about the variability, both in the population and the inter-person biological variability. So it's a good idea to both take technical replicates, that is two, run two experiments using the exact same sample to try to measure how well your technology works, and biological replicates. Replicates where you take it from different individuals so you can take it from different individuals so you can measure the inter-person biological variability. Good designs also have controls, both negative and positive controls, to make sure that both your technology is working and that any effects that you've detected aren't just due to an artifact of the computation or artifact of the experimental design.

# Python for Genomic Data Science

## Biopython 

https://biopython.org/


# Algorithms for DNA Sequencing

Module 1: DNA sequencing, strings and matching: Why study this? DNA sequencers and how they work. How DNA can be represented as a string. Using Python to parse and manipulate real genome sequences and real DNA sequencing data.  Naive exact matching.

Module 2: Preprocessing, indexing and approximate matching: Improving on naive exact matching with Boyer-Moore.  Preprocessing and indexing.  Indexing: grouping and ordering.  k-mers and k-mer indexes.  Approximate matching and the pigeonhole principle.

Module 3: Edit distance, assembly, overlaps: Hamming and edit distance. Algorithms for computing edit distance. Dynamic programming. Global and local alignment. De novo assembly. Overlaps and overlap graphs.

Module 4: Algorithms for assembly: Shortest common superstring and the greedy version. How repetitive DNA makes assembly difficult. De Bruijn graphs and Eulerian walks. How real assemblers work. The future of assembly. Wrap up.

## Genomes as strings, reads as substring

DNA is the kind of molecule that encodes your genome, the sum total of all your genetic information, all your genes. So let me explain this with an analogy. Your genome is sort of like a book of recipes, with a separate recipe for each type of molecule in your body. Each little physical piece that makes up your brain cells and your skin cells and your heart cells, etc. These are the machines that do the work of building and maintaining you.

This recipe book is not written in English. It's written in a different language that you've probably seen before, where the alphabet consists of the four letters A, C, G, and T. These letters stand for different kinds of molecules, different bases. A for adenine, C for cytosine, G for guanine, and T for thymine.

A DNA molecule is shaped like a double helix, this thing that looks like a twisted ladder. And the rungs of this ladder are made up of pairs of bases. Specifically, these are complementary base pairs. A is complementary to T and vice versa. And C is complementary to G and vice versa. And so if we look at one of the rungs of this ladder here, for instance, we see two colors, orange and red, which correspond to the two complementary bases, C and G. If we wanted to write down the sequence of bases that describe this molecule, we might do it like this. We might start all the way up at the top and then work our way down one side of the ladder, reading off each base as we go.

So if we do this in this way, then we can take the DNA molecule and turn it into a sequence of letters, a string.

And the fact that we can write a DNA molecule as a string has profound implications for how we can analyze it. And we'll return to this point later.

Now really, DNA molecules are much longer than what I'm showing here on this slide. Human chromosomes, for example, are on the order of hundreds of millions of bases long. But even so, we can still think of these chromosomes as strings. They're very, very long strings, but they're still strings.

So, here for example is a tiny snippet of the human genome. Again we're writing it as a string, and this string wraps around from one line to the next, sort of like if you were reading a book. So you would start in the upper left and then read the first line, and then go down to the next line, etc. Now when we look at this string, we don't really understand it, right, we don't know what this means. Is this one of those recipes we talked about before? Is this maybe many recipes together? It's not really clear. In fact, there is one gene in the middle of this sequence here, which I've highlighted here in red, and this gene is called HBB. And you can see it's spread across the genome in a few different pieces.
So, we can sequence this short bit of DNA just sort of by eye, by looking at the colors of the rungs of this ladder. So how does a DNA sequencer sequence a genome?

A crucial point is that DNA sequencers are not actually very good at reading long stretches of DNA. They're very good at reading short stretches of DNA, but lots and lots and lots of them. So this is what DNA sequencers do well. They read lots of short stretches of DNA. So let's look at an illustration.

We start out with the DNA that we'd like to sequence, represented by this string at the bottom of the slide here. This is the input DNA. This input DNA might be, for example, your genome. And the DNA sequencer works by repeatedly reading off randomly selected substrings from the input DNA, randomly selected snippets out from the middle of the input DNA, many, many, many, many of them.
So here are many of them.

These snippets are called sequencing reads, or simply reads for short.
But these reads are themselves very, very short, compared to the length of the input DNA. So, for example, like we said before, one human chromosome is on the order of about a hundred-million bases long. But massively parallel sequencers are these second generation sequencers,

produce reads that are closer to about 150, or a couple hundred basis long, or so. So these reads are many orders of magnitude shorter than the input DNA.

But the good news is that we get lots, and lots, and lots of these short snippets of DNA. Usually we have enough of these reads to cover the whole genome over many times over. In other words, we have redundant information about any given base of the genome that we are sequencing.

## Naive exact matching algorithm

Naive pattern searching is the simplest method among other pattern searching algorithms. It checks for all character of the main string to the pattern.
Naive algorithm is exact string matching(means finding one or all exact occurrences of a pattern in a text) algorithm.
This algorithm is helpful for smaller texts. It does not need any pre-processing phases. We can find substring by checking once for the string. It also does not occupy extra space to perform the operation.
The naive approach tests all the possible placement of Pattern P [1…….m] relative to text T [1……n]. We try shift s = 0, 1…….n-m, successively and for each shift s. Compare T [s+1…….s+m] to P [1……m].It returns all the valid shifts found.

see more (here)[https://www.geeksforgeeks.org/naive-algorithm-for-pattern-searching/]

## Knuth–Morris–Pratt algorithm

KMP算法是一种字符串匹配算法，可以在 O(n+m) 的时间复杂度内实现两个字符串的匹配。

* https://www.zhihu.com/question/21923021


## Boyer-Moore string-search algorithm

So far we've seen the naive exact matching algorithm, which isn't particularly fast, and which isn't able to find approximate matches, but a useful solution to the read alignment problem will have to be both fast and approximate. So in this module, we'll push in both directions. First, we'll discuss a very practical and a very fast and fairly simple alternative to the naive exact matching algorithm, which is called Boyer-Moore. 

In computer science, the Boyer–Moore string-search algorithm is an efficient string-searching algorithm that is the standard benchmark for practical string-search literature.[1] It was developed by Robert S. Boyer and J Strother Moore in 1977.[2] The original paper contained static tables for computing the pattern shifts without an explanation of how to produce them. The algorithm for producing the tables was published in a follow-on paper; this paper contained errors which were later corrected by Wojciech Rytter in 1980.

The algorithm preprocesses the string being searched for (the pattern), but not the string being searched in (the text). It is thus well-suited for applications in which the pattern is much shorter than the text or where it persists across multiple searches. The Boyer–Moore algorithm uses information gathered during the preprocess step to skip sections of the text, resulting in a lower constant factor than many other string search algorithms. In general, the algorithm runs faster as the pattern length increases. The key features of the algorithm are to match on the tail of the pattern rather than the head, and to skip along the text in jumps of multiple characters rather than searching every single character in the text.

## Edit distance

In computational linguistics and computer science, edit distance is a string metric, i.e. a way of quantifying how dissimilar two strings (e.g., words) are to one another, that is measured by counting the minimum number of operations required to transform one string into the other. Edit distances find applications in natural language processing, where automatic spelling correction can determine candidate corrections for a misspelled word by selecting words from a dictionary that have a low distance to the word in question. In bioinformatics, it can be used to quantify the similarity of DNA sequences, which can be viewed as strings of the letters A, C, G and T.

Different definitions of an edit distance use different sets of string operations. Levenshtein distance operations are the removal, insertion, or substitution of a character in the string. Being the most common metric, the term Levenshtein distance is often used interchangeably with edit distance

# Bioconductor for Genomic Data Science

https://www.bioconductor.org/

## GRanges 


Hi and welcome to an overview of GRanges. In this and the following videos, we are going to discuss GRanges, which is a data structure for storing genomic intervals in R. The key thing about GRanges is that, they're fast and efficient. And I think, it's fair to say that thry have completely transformed my own work. This is data structures that I use almost every day at work with genomic data.

In my opinion, every R user dealing with genomic data needs to master these data structures. And the functionality, provided by these data structures in order to facilitate their own work.

So, the key insight is that many integers in genomics can be thought of intervals or perhaps sets of intervals of integers. So here's a screenshot from the UCSC genome browser. It's a somewhat randomly chosen gene. And we can see that genes intervals. They are DNA's clusters. They are SNPs. They are repeat mask regions of the genome. And basically many, many, many integers in genomics can be thought of as intervals. Promoters, Genes, single-nucleotide polymorphism, which are really intervals, but consist of only a single base, CpG Islands. But also, data, such as next-generation sequencing reads after they've been mapped. Once a reader's been mapped to the genome, it's an interval. All sequence data that has been processed is at fault. Perhaps, you have done a gypsy experiment and you've done pcoding and you end up with peaks that are often described as intervals with some score associated to them.

The course may do objects, can be thought of in intervals. Many tasks in genomics involves relating sets of intervals to each other. For example, question subsets, which promoters contain SNPs? Which transcription facts are bindings sites overlap a promoter? Which genes are covered by sequencing reads? These are all a task that we do again and again. And conceptually, it 's about relating different sets of intervals to each other. And this is the kind of functionality, that's provided by the framework we're discussing here. So here's a little visual depiction of some r output of the GRanges. A GRange, as you can see here, consists of three, this particular GRange, consists of three Genomic intervals.

The three intervals are all on chromosome one. They have a strand associated on them which in this case is plus, minus, and plus. And they have some ranges. The first interval goals contains the bases one, two, and three. And the second one contains three, four, and five. The GRanges has names associated with it. In this particular instance, that's optional. The names are A1, A2, and A3. And there's also some information about the genome,

which is not very helpful in this case here. In this case, the software has inferred that there's a single chromosome. And it doesn't know how long the chromosome is. But usually, when we work with human data. We know exactly how long the different chromosomes are. And many people like to store this information and the object as well. So, GRanges are defined and the functionality is provided by two fairly complicated packages called GenomicRanges and another package called IRanges.

As I said before, these packets are fast and efficient. But at first glance, they can appear very complicated with many different classes and a lot of different functions. What we are going to try to do in the following videos is simplify it a little bit to make it easier to process.

GRanges, or genomic ranges, as a software package is described in this excellent paper from Mike Lawrence and other authors in PLoS Computational Biology. And what we're doing, or this concept of computing on intervals. Is something that is functional that is also, provided by a command line tool called bit tools which is an excellent tool. It's not really something we are going to discuss in this series of lectures.

# Statistics for Genomic Data Science

* Dimension Reduction
* Pre-processing and Normalization
  + The first statistical step in almost any genomic data analysis is pre-processing and normalization. The basic idea is that the data come in in a raw format that's very complicated, or maybe too big, or maybe too unprocessed. And so your step is to process that data, get it normalized, get it set up and situated. It's easy to perform statistical analysis.
* Batch Effects and Confounders
  + When it's used by biologists and genomic scientists, they often talk about the batch that the samples were processed in, so the time in which a group of samples that went through together, or the slide that they went through together on. But it turns out that batch effects is a surrogate for lots of different confounders. So there could be external factors like the environment that might affect the level of genomic measurements, there could be genetic or epigenetic factors that contribute to the overall expression of a gene, or the overall epigenetic profile. And then there could be technological factors. So for example, if you have a diligent scientist versus a crazy scientist working away and doing an experiment, you might get different results. So all of these things are confounders that you might need to adjust for when doing your experiment. So here's a quick example of that. So I've taken three studies, you can find the results of these studies in these different papers that I've linked to here. And so I colored by different things. So, in this case, I've done a clustering, and I colored by the environment. And so you can see that the orange environment sort of clusters together into these two clusters. Here, I colored by processing year and here you can see that the purple processing year clusters together. And then here, in this study, I've actually clustered the data just by a particular allele, and so here, you can see that this allele, the orange case, they cluster together. So these are expression measurements that cluster together by different variables. Any one of these could be a confounder or a batch effect that you have to adjust for If it's not the variable of interest in your study. So this doesn't just affect the continuous measurements like gene expression measurements. This is actually data from the 1000 Genomes Project, so this is a particular genomic location between this base pair and this base pair. And so you can see the samples are ordered by date here. And this is after normalizing the samples to have the same read coverage for all the different samples on average, so the global distributions are the same. You can see that there's still a set of samples here that seem to have a much higher level of coverage than the set of samples that were processed on a different date here for this region. And so this sort of batch effect appears in lots of different types of genomic data.
  + https://www.nature.com/articles/nrg2825
  + https://academic.oup.com/biostatistics/article/8/1/118/252073
* Permutation

## Importance P Value

p-values almost always go to zero with the sample size. That's another common misinterpretation of the p-value. Just because you got a really small p-value, it doesn't mean that the difference is huge. It could just be that your sample size is really large, and so the variability is small. Even if you have any difference at all, as the sample size gets big, the p-value will get small. The usual cut off that people use for calling p-values significant is 0.05. This is if you're doing only a single hypothesis test, but that number is basically just a made up number. So it could be any other threshold could also be used. I mean it's useful to have a standard, but don't treat this as sort of religious truth that 0.05 is the right way to tell if your p-value significant. And you should always report p-values in conjunctions with estimates and variances on the scale that's scientifically meaningful. P-values can be useful as a complement to that, as a way to sort of quantify statistical significance, as long as you pay attention to the properties of the p-values and interpret them correctly.


 