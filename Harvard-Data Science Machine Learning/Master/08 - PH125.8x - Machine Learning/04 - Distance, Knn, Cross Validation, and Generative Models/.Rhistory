knitr::opts_chunk$set(echo = TRUE)
library(dbplyr)
library(dslabs)
library(tidyverse)
library(caret)
set.seed(1996)
n <- 1000
p <- 10000
x <- matrix(rnorm(n*p), n, p)
colnames(x) <- paste("x", 1:ncol(x), sep = "_")
y <- rbinom(n, 1, 0.5) %>% factor()
x_subset <- x[ ,sample(p, 100)]
fit <- train(x_subset, y, method = "glm")
fit$results
library(MASS)
data(biopsy)
biopsy$ID = NULL
names(biopsy) = c("thick", "u.size", "u.shape", "adhsn",
"s.size", "nucl", "chrom", "n.nuc", "mit", "class")
biopsy.v2 <- na.omit(biopsy)
## 为了满 足这个要求，可以创建一个变量y，用0表示良性，用1表示恶性
## 使用ifelse()函数为y赋 值，如下所示:
y <- ifelse(biopsy.v2$class == "malignant", 1, 0)
set.seed(123) #random number generator
ind <- sample(2, nrow(biopsy.v2), replace = TRUE, prob = c(0.7, 0.3))
train <- biopsy.v2[ind==1, ] #the training data set
test <- biopsy.v2[ind==2, ] #the test data set
trainY <- y[ind==1]
testY <- y[ind==2]
lda.fit <- lda(class ~ ., data = train)
lda.fit
library(tidyverse)
library(caret)
library(dslabs)
data("heights")
y <- heights$height
set.seed(1995)
test_index <- createDataPartition(y, times = 1, p = 0.5, list = FALSE)
train_set <- heights %>% slice(-test_index)
test_set <- heights %>% slice(test_index)
x <- test_set$height
f0 <- dnorm(x, params$avg[2], params$sd[2])
params <- train_set %>%
group_by(sex) %>%
summarize(avg = mean(height), sd = sd(height))
params
## prevalence, which we will denote with  π=Pr(Y=1),
pi <- train_set %>% summarize(pi=mean(sex=="Female")) %>% pull(pi)
pi
## Now we can use our estimates of average and standard deviation to get an actual rule:
x <- test_set$height
f0 <- dnorm(x, params$avg[2], params$sd[2])
f1 <- dnorm(x, params$avg[1], params$sd[1])
p_hat_bayes <- f1*pi / (f1*pi + f0*(1 - pi))
p_hat_bayes
qplot(x,p_hat_bayes)
?qplot
qplot(x,p_hat_bayes,xlab ="x", ylab="prob" )
y_hat_bayes <- ifelse(p_hat_bayes > 0.5, "Female", "Male")
sensitivity(data = factor(y_hat_bayes), reference = factor(test_set$sex))
specificity(data = factor(y_hat_bayes), reference = factor(test_set$sex))
p_hat_bayes_unbiased <- f1 * 0.5 / (f1 * 0.5 + f0 * (1 - 0.5))
y_hat_bayes_unbiased <- ifelse(p_hat_bayes_unbiased> 0.5, "Female", "Male")
sensitivity(factor(y_hat_bayes_unbiased), factor(test_set$sex))
specificity(factor(y_hat_bayes_unbiased), factor(test_set$sex))
qplot(x, p_hat_bayes_unbiased, geom = "line") +
geom_hline(yintercept = 0.5, lty = 2) +
geom_vline(xintercept = 67, lty = 2)
data("mnist_27")
params <- mnist_27$train %>%
group_by(y) %>%
summarize(avg_1 = mean(x_1), avg_2 = mean(x_2),
sd_1= sd(x_1), sd_2 = sd(x_2),
r = cor(x_1, x_2))
params
### 绘制数据并使用等高线图来了解两个估计的正常密度的样子（我们显示的曲线代表一个包含 95% 点的区域）
mnist_27$train %>% mutate(y = factor(y)) %>%
ggplot(aes(x_1, x_2, fill = y, color=y)) +
geom_point(show.legend = FALSE) +
stat_ellipse(type="norm", lwd = 1.5)
library(caret)
train_qda <- train(y ~ ., method = "qda", data = mnist_27$train)
### We see that we obtain relatively good accuracy:
y_hat <- predict(train_qda, mnist_27$test)
confusionMatrix(y_hat, mnist_27$test$y)$overall["Accuracy"]
train_lda <- train(y ~ ., method = "lda", data = mnist_27$train)
y_hat <- predict(train_lda, mnist_27$test)
confusionMatrix(y_hat, mnist_27$test$y)$overall["Accuracy"]
train_lda <- train(y ~ ., method = "knn", data = mnist_27$train)
y_hat <- predict(train_lda, mnist_27$test)
confusionMatrix(y_hat, mnist_27$test$y)$overall["Accuracy"]
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(dslabs)
mnist <- read_mnist()
dim(mnist$train$images)
set.seed(1990)
index <- sample(nrow(mnist$train$images), 10000)
x <- mnist$train$images[index,]
y <- factor(mnist$train$labels[index])
index <- sample(nrow(mnist$test$images), 1000)
x_test <- mnist$test$images[index,]
y_test <- factor(mnist$test$labels[index])
library(matrixStats)
sds <- colSds(x)
qplot(sds, bins = 256)
library(caret)
nzv <- nearZeroVar(x)
## We can see the columns recommended for removal:
image(matrix(1:784 %in% nzv, 28, 28))
rafalib::mypar()
image(matrix(1:784 %in% nzv, 28, 28))
col_index <- setdiff(1:ncol(x), nzv)
length(col_index)
colnames(x) <- 1:ncol(mnist$train$images)
colnames(x_test) <- colnames(x)
n <- 1000
b <- 2
index <- sample(nrow(x), n)
control <- trainControl(method = "cv", number = b, p = .9)
train_knn <- train(x[index, col_index], y[index],
method = "knn",
tuneGrid = data.frame(k = c(3,5,7)),
trControl = control)
## 然后我们可以增加 n 和 b 并尝试建立它们如何影响计算时间的模式，以了解对于较大的 n 和 b 值拟合过程需要多长时间。 您想知道一个函数在运行之前是否需要数小时甚至数天。
## 一旦我们优化了我们的算法，我们就可以将它拟合到整个数据集：
fit_knn <- knn3(x[, col_index], y,  k = 3)
y_hat_knn <- predict(fit_knn, x_test[, col_index], type="class")
cm <- confusionMatrix(y_hat_knn, factor(y_test))
cm$overall["Accuracy"]
## 从特异性和敏感性来看，我们还看到 8 是最难检测到的，最常被错误预测的数字是 7
cm$byClass[,1:2]
library(randomForest)
control <- trainControl(method="cv", number = 5)
grid <- data.frame(mtry = c(1, 5, 10, 25, 50, 100))
train_rf <-  train(x[, col_index], y,
method = "rf",
ntree = 150,
trControl = control,
tuneGrid = grid,
nSamp = 5000)
### Now that we have optimized our algorithm, we are ready to fit our final model:
fit_rf <- randomForest(x[, col_index], y,
minNode = train_rf$bestTune$mtry)
### To check that we ran enough trees we can use the plot function:
plot(fit_rf)
models <- c("glm", "lda", "naive_bayes", "svmLinear", "knn", "gamLoess", "multinom", "qda", "rf", "adaboost")
set.seed(1, sample.kind = "Rounding")
data("mnist_27")
fits <- lapply(models, function(model){
print(model)
train(y ~ ., method = model, data = mnist_27$train)
})
names(fits) <- models
### dimensions of the matrix of predictions
pred <- sapply(fits, function(object)
predict(object, newdata = mnist_27$test))
dim(pred)
### compute accuracy for each model on the test set.
acc <- colMeans(pred == mnist_27$test$y)
View(pred)
### build an ensemble prediction by majority vote and compute the accuracy of the ensemble. Vote 7 if more than 50% of the models are predicting a 7, and 2 otherwise.
votes <- rowMeans(pred == "7")
head(votes)
ind <- acc > mean(y_hat == mnist_27$test$y)
sum(ind)
mean(y_hat == mnist_27$test$y)
set.seed(1, sample.kind="Rounding") # if using R 3.6 or later
scores <- sapply(1:nrow(schools), function(i){
scores <- rnorm(schools$size[i], schools$quality[i], 30)
scores
})
mu <- round(80 + 2*rt(1000, 5))
scores <- sapply(1:nrow(schools), function(i){
scores <- rnorm(schools$size[i], schools$quality[i], 30)
scores
})
schools <- data.frame(id = paste("PS",1:1000),
size = n,
quality = mu,
rank = rank(-mu))
schools %>% top_n(10, score) %>% arrange(desc(score)) %>% dplyr::select(id, size, score)
scores <- sapply(1:nrow(schools), function(i){
scores <- rnorm(schools$size[i], schools$quality[i], 30)
scores
})
schools %>% top_n(10, score) %>% arrange(desc(score)) %>% dplyr::select(id, size, score)
schools <- schools %>% mutate(score = sapply(scores, mean))
u <- round(80 + 2*rt(1000, 5))
scores <- sapply(1:nrow(schools), function(i){
scores <- rnorm(schools$size[i], schools$quality[i], 30)
scores
})
schools <- schools %>% mutate(score = sapply(scores, mean))
schools %>% top_n(10, score) %>% arrange(desc(score)) %>% dplyr::select(id, size, score)
median(schools$size)
set.seed(1, sample.kind="Rounding") # if using R 3.6 or later
mu <- round(80 + 2*rt(1000, 5))
schools <- data.frame(id = paste("PS",1:1000),
size = n,
quality = mu,
rank = rank(-mu))
median(schools$size)
scores <- sapply(1:nrow(schools), function(i){
scores <- rnorm(schools$size[i], schools$quality[i], 30)
scores
})
schools <- schools %>% mutate(score = sapply(scores, mean))
scores <- sapply(1:1000, function(i){
scores <- rnorm(schools$size[i], schools$quality[i], 30)
scores
})
schools <- schools %>% mutate(score = sapply(scores, mean))
# set.seed(1) # if using R 3.5 or earlier
set.seed(1, sample.kind="Rounding") # if using R 3.6 or later
mu <- round(80 + 2*rt(1000, 5))
scores <- sapply(1:nrow(schools), function(i){
scores <- rnorm(schools$size[i], schools$quality[i], 30)
scores
})
schools <- schools %>% mutate(score = sapply(scores, mean))
View(schools)
scores <- sapply(1:nrow(schools), function(i){
scores <- mean(rnorm(schools$size[i], schools$quality[i], 30))
scores
})
schools <- schools %>% mutate(score = sapply(scores, mean))
View(schools)
schools %>% top_n(10, score) %>% arrange(desc(score)) %>% dplyr::select(id, size, score)
